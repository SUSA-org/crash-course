{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUSA CX Kaggle Capstone Project\n",
    "## Part 3: Hyperparameter Tuning, Decision Trees, Ensemble Learning\n",
    "\n",
    "### Table Of Contents\n",
    "* [Introduction](#section1)\n",
    "* [Hyperparameters](#section2)\n",
    "* [GridSearch](#section2i)\n",
    "* [Decision Trees](#section3)   \n",
    "* [Random Forest](#section4)\n",
    "* [Ensemble Learning](#section5)\n",
    "* [Conclusion](#conclusion)\n",
    "* [Additional Reading](#reading)\n",
    "\n",
    "\n",
    "### Hosted by and maintained by the [Statistics Undergraduate Students Association (SUSA)](https://susa.berkeley.edu). Originally authored by [Patrick Chao](mailto:prc@berkeley.edu) & [Noah Gundotra](mailto:noah.gundotra@berkeley.edu)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "# SUSA CX Kaggle Capstone Project\n",
    "\n",
    "Welcome to the second week of the SUSA CX Kaggle Capstone! Now that you have an understanding of your data, a cleaned dataset, and a working model, now it's time to improve your model performance with some new tricks and techniques. Now that we have explored regularization techniques, PCA for feature selection and dimension reduction, and feature engineering, we will delve into more technical details and advanced models.\n",
    "\n",
    "\n",
    "\n",
    "## Logistics\n",
    "\n",
    "Most of the logistics are the same as last week, but we are repeating them here for your convenience. Please let us know if you or your teammates are feeling nervous about the pace of this project - remember that we are not grading you on your project, and we really try to make the notebooks relatively easy and fast to code through. If for any reason you are feeling overwhelmed or frustrated, please DM us or talk to us in person. We want all of you to have a productive, healthy, and fun time learning data science! If you have any suggestions or recommendations on how to improve, please do not hesitate to reach out!\n",
    "\n",
    "## Datathon\n",
    "\n",
    "For those of you that came to the Datathon, we hope you all had a great time! It seemed that many people got a lot of work done and learned a ton of new materials. We had a quick lecture by Patrick on the mathematical intuitions on regularization and PCA. If you couldn't make it or didn't understand all the details, feel free to reach out! Also if you felt that the lecture was very helpful and you would like to see more similar stuff, let us know!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mandatory Office Hours\n",
    "\n",
    "Because this is such a large project, you and your team will surely have to work on it outside of meetings. In order to get you guys to seek help from this project, we are making it **mandatory** for you and your group to attend **two (2)** SUSA Office Hours over the next 4 weeks. This will allow questions to be answered outside of the regular meetings and will help promote collaboration with more experienced SUSA members.\n",
    "\n",
    "The schedule of SUSA office hours are below:\n",
    "https://susa.berkeley.edu/calendar#officehours-table\n",
    "\n",
    "We understand that most of you will end up going to Arun or Patrick's office hours, but we highly encourage you to go to other people's office hours as well. There are many qualified SUSA mentors who can help and this could be an opportunity for you to meet them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pydotplus'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-21dbd183f970>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexport_graphviz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpydotplus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0msqrt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pydotplus'"
     ]
    }
   ],
   "source": [
    "# Import statements\n",
    "from sklearn import tree # There are lots of other models from this module you can try!\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression, Ridge\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "sqrt=np.sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "# Hyperparameters and Tuning\n",
    "\n",
    "As we have discussed in the project workflow, we are beginning to investigate more technical models. However, as we continue to explore more and more complex models, we may lose interpretability. Many problems in machine learning or data science eventually reduce to some parameters we are incapable of understanding or predicting ahead of time. These may be parameters inherent to the dataset or based on the model.\n",
    "\n",
    "If we look back at k-Nearest Neighbors, based on different values of $k$ we obtain various models. For $k=1$, we obtain a low bias and high variance estimate. Our estimate is highly dependent on the data available. As $k$ increases, our bias increases but the variance decreases. We would like to minimize both bias and variance, thus the optimal value of $k$ lies somewhere between these values. How do we determine this value? Unfortunately there is no golden bullet or magical formula we may calculate to answer this question.\n",
    "\n",
    "These mystical quantities we would like to evaluate are known as **hyperparameters**. Hyperparameters are values in our model that we cannot optimize directly, instead we may have to explore a variety of examples before determining the optimal value. In kNN, the value of $k$ characterizes the model. Thus we may describe this as a **model hyperparameter**. If we were using polynomial regression, the degree of the model would also be a model hyperparameter, as we can arbitrarily select the power and it characterizes our model.\n",
    "\n",
    "Another example of a hyperparameter is the $\\lambda$ used in Ridge and LASSO regression. There is essentially no method of evaluating the optimal value of $\\lambda$ through calculations, instead we must experiment and emperically determine the best value. These do not determine the form of model, but affect the optimization. Thus this is an **optimization hyperparameter**.\n",
    "\n",
    "We may imagine hyperparameters as a bunch of individual knobs we may turn. Consider that we are visiting our friend and staying at her place. However, you did not realize that she is actually an alien and her house is filled with very strange objects. When you head to bed, you attempt to use her shower, but see that her shower is has a dozen of knobs that control the temperature of the water coming out! We only have a single output to work off of, but many different knobs or *parameters* to adjust. If the water is too hot, we can turn random knobs until it becomes cold, and learn a bit about our environment. We may determine that some knobs are more or less sensitive, just like like hyperparameters. Each knob in the shower is equivalent to a hyperparameter we can tune in a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2i'></a>\n",
    "# Grid Search\n",
    "\n",
    "Consider we have two hyperparameters $A$ and $B$ that range from $0$ to $100$. \n",
    "\n",
    "> 1. Discuss with a group on possible methods of deciding the optimal value of $A$ and $B$?\n",
    "> 2. Is it possible to find the exact best values of $A$ and $B$?\n",
    "\n",
    "One of the most naive approaches is to simply create a grid of values for $A$ and $B$, hence the name grid search. We could consider choosing the values where $A$ or $B$ are equal to $0,50,100$, and determine the validation error on all pairs of hyperparameters. Since there are $3$ values for both $A$ and $B$, there are $9$ total possible choices for $(A,B)$.\n",
    "\n",
    "Consider the following grid below.\n",
    "\n",
    "<img src=\"GRAPHICS/grid1.png\" width=\"30%\">\n",
    "\n",
    "The grid represents the validation error for all $(A,B)$ from $[0,50,100]$. It may seem that $(A,B)=(50,100)$ is the best value with an error of $7$, but our scale is quite coarse. We could instead have four values for $A$ and $B$, resulting in $16$ total calculations. This is shown below with $[0,33,66,100]$.\n",
    "<img src=\"GRAPHICS/grid2.png\" width=\"30%\">\n",
    "\n",
    "Now the optimal value shifts to $(A,B)=(66,33)$ with an error of $7$, but this will change again as our interval becomes more and more fine. However, a few issues immediately arise. First, the number of calculations increases quadratically with the number of values for each interval! This is quite bad, as quadratic functions grow quite quickly. Secondly, we can never really determine the exact optimal value unless we had infinitely fine intervals. Lastly, we only had an decrease in error from $7$ to $5$ with $9$ to $16$ calculations. Is this actually worthwhile? Sometimes performing more searches does not actually help. These are all various tradeoffs to decide, at a certain point the number of calculations is not worth the added benefit of well tuned parameters.\n",
    "\n",
    "Now, this is only for the two variable case with $A$ and $B$. In practice, models may have many variables, even dozens! If we consider $10$ parameters, just doing two values for an interval results in $2^10=1024$ calculations! Keep in mind that each value in the grid requires a separate model to be trained using the parameters, which results in $1024$ different models to be trained with just a comparison of two values for each parameter! The number of models trained is equal to $m^n$ where $m$ is the number of values in the interval, and $n$ is the number of hyperparameters.\n",
    "\n",
    "Grid search can be very challenging due to this issue, and suffers from the curse of dimensionality. There are other approaches that are used in practice. Instead of methodically selecting all pairs of values in the cartesian product, we could perform a [random search](https://en.wikipedia.org/wiki/Random_search) through parameters. Even more complex, we can create a model to estimate the optimal parameters. This leads into the field of [meta-learning](http://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/), learning to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "# Decision Trees with Grid Search\n",
    "\n",
    "Now that we have investigated how hyperparameters affect our models, we may apply it directly to our current problem of predicting housing prices. In this section, we will be going over decision trees. If you would like a refresher, please refer to this [notebook](../Machine%20Learning/BiasVarianceDecisionTrees/Bias%20Variance%20Decision%20Trees%20Ensemble%20Learning.ipynb)!\n",
    "\n",
    "In the past, we talked about decision trees as purely classification. In practice, we can also use decision trees for regression! Rather than have a single class at the leaves and attempting to reduce the impurity or entropy of individual leaves, we have individual functions at the leaves of the decision tree. \n",
    "\n",
    "First, we will load in the cleaned data and import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features(data, col_list, y_name):\n",
    "    \"\"\"\n",
    "    Function to return a numpy matrix of pandas dataframe features, given k column names and a single y column\n",
    "    Outputs X, a n X k dimensional numpy matrix, and Y, an n X 1 dimensional numpy matrix.\n",
    "    This is not a smart function - although it does drop rows with NA values. It might break. \n",
    "    \n",
    "    data(DataFrame): e.g. train, clean\n",
    "    col_list(list): list of columns to extract data from\n",
    "    y_name(string): name of the column you to treat as the y column\n",
    "    \n",
    "    Ideally returns one np.array of shape (len(data), len(col_list)), and one of shape (len(data), len(col_list))\n",
    "    \"\"\"\n",
    "    \n",
    "    # keep track of numpy values\n",
    "    feature_matrix = data[col_list + [y_name]].dropna().values\n",
    "    np.random.shuffle(feature_matrix)\n",
    "    return feature_matrix[:, :-1], feature_matrix[:, -1]\n",
    "\n",
    "def get_loss(model, X,Y_true):\n",
    "    \"\"\"Returns square root of L2 loss (RMSE) from a model, X value input, and true y values\n",
    "    \n",
    "    model(Model object): model we use to predict values\n",
    "    X: numpy matrix of x values\n",
    "    Y_true: numpy matrix of true y values\n",
    "    \"\"\"\n",
    "    Y_hat = model.predict(X)\n",
    "    return get_RMSE(Y_hat,Y_true)\n",
    "\n",
    "def get_RMSE(Y_hat,Y_true):\n",
    "    \"\"\"Returns square root of L2 loss (RMSE) between Y_hat and true values\n",
    "    \n",
    "    Y_true: numpy matrix of predicted y values\n",
    "    Y_true: numpy matrix of true y values\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.mean((Y_true-Y_hat)**2))\n",
    "\n",
    "def get_train_and_val(X,Y):\n",
    "    \"\"\"Given the X and Y data, return the training and validation based on the split variable\n",
    "    \n",
    "    X: numpy matrix of x values\n",
    "    Y: numpy matrix of y values\n",
    "    split: value between 0 and 1 for the training split\n",
    "    \"\"\"\n",
    "    \n",
    "    Y = Y.reshape(Y.shape[0],)\n",
    "\n",
    "    train_index,_ = get_train_val_indices(X,Y)\n",
    "\n",
    "    y_train = Y.reshape(Y.shape[0],)\n",
    "    y_train = Y[:train_index]\n",
    "    x_train = X[:train_index,:]\n",
    "\n",
    "    x_val = X[train_index:,:]\n",
    "    y_val = Y[train_index:]\n",
    "    return (x_train,y_train),(x_val,y_val)\n",
    "\n",
    "def get_train_val_indices(X,Y=None,split=0.7):\n",
    "    train_index = (int)(X.shape[0]*split)\n",
    "    test_index =X.shape[0]-1\n",
    "    return train_index,test_index\n",
    "\n",
    "def grid_search(model,grid_params,verbose=True):\n",
    "    \"\"\" Given a model and a set of parameters, this will search through the grid of parameters\n",
    "    \n",
    "    model: model input ex: tree.DecisionTreeRegressor()\n",
    "    grid_params: dictionary of parameter values, parameter to list\n",
    "    verbose: boolean default to true, prints the value of all scores in the grid\n",
    "    \n",
    "    Modified from http://scikit-learn.org/0.15/auto_examples/grid_search_digits.html\n",
    "    \"\"\"\n",
    "\n",
    "    clf=GridSearchCV(model,param_grid = grid_params,n_jobs=20,cv=3,scoring=make_scorer(get_RMSE,greater_is_better=False))\n",
    "    clf.fit(X,Y)\n",
    "    if verbose:\n",
    "        print(\"Grid scores on development set:\")\n",
    "        for params, mean_score, scores in clf.grid_scores_:\n",
    "            print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "                  % (-1*mean_score, scores.std() / 2, params))\n",
    "        print()\n",
    "    print(\"Best parameters set found:\")\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Best score found:\")\n",
    "    print(-1*clf.best_score_)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the code above is the same as last week with a few tweaks and added methods. A few important changes: now the training and validation set is randomized, so you may obtain different losses based on different random initalizations.\n",
    "\n",
    "Replace `feature_cols` with the features you chose in the previous notebook. For decision trees, we would recommend having more features rather than fewer, recommended to have at least $10$. \n",
    "\n",
    "We will first read in the data, select the relevant features, and separate into training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = pd.read_csv('DATA/house-prices/train_cleaned.csv')\n",
    "#feature_cols = ['LotArea', 'Utilities', 'OverallCond', 'BsmtFinSF1', 'MasVnrArea']\n",
    "\n",
    "def select_columns_except(dframe, non_examples):\n",
    "    \"\"\"Returns all comlumns in dframe except those in non_examples.\"\"\"\n",
    "    all_cols = dframe.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cond = lambda x: sum([x == col for col in non_examples]) >= 1\n",
    "    return [x for x in all_cols if not cond(x)]\n",
    "    \n",
    "feature_cols = select_columns_except(clean, ['Id','SalePrice'])\n",
    "\n",
    "X, Y = get_features(clean, feature_cols, 'SalePrice')\n",
    "(x_train,y_train),(x_val,y_val) = get_train_and_val(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using sklearn, we will train a decision tree regressor with all default parameters. Keep in mind since we have a small dataset and our decision tree may vary wildly with each optimization, the RMSE will also change greatly from each time you run it! This dataset is particularly small so it does not fair well with very deep trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error loss of our model: 38409.14\n"
     ]
    }
   ],
   "source": [
    "decisionTree = tree.DecisionTreeRegressor()\n",
    "decisionTree = decisionTree.fit(x_train, y_train)\n",
    "loss = get_loss(decisionTree, x_val,y_val)\n",
    "print(\"Root Mean Squared Error loss of our model: {:.2f}\".format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will experiment with $3$ hyperparameters in the decision tree sklearn implementation. There are more than $3$ hyperparameters, but we will stick with three for simplicity.\n",
    "\n",
    "The three we are using are `max_depth`, `min_samples_split`, and `min_samples_leaf`. \n",
    "> `max_depth`: The maximum depth of any branch in the tree. As max_depth increases, our tree gets more and more complex and is more likely to overfit to the data.    \n",
    "> `min_samples_split`: Since each branch can continue splitting to obtain more and more pure leaves, eventually it may not make sense to continue splitting as we may be overfitting. To avoid this, we may set a min_samples_split, which represents the minimum number of samples to split on.     \n",
    "> `min_samples_leaf`: We can continue our decision tree until the leaves become very small and we overfit. Another restraint could be to have a minimum value restricting the number of samples in a leaf. \n",
    "\n",
    "The code below runs a grid search on the parameters chosen. Feel free to modify the ranges for all the hyperparameters! This is a crucial part of model selection. \n",
    "\n",
    "The `grid_search` function takes in a model, a dictionary of a set of parameters, and a boolean flag for verbose. The model in our case will just be tree.DecisionTreeRegressor(), and the parameters is equal to `grid_params`. Feel free to change the verbose flag to True if you would like to view all the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found:\n",
      "{'max_depth': 10, 'min_samples_leaf': 12, 'min_samples_split': 10}\n",
      "\n",
      "Best score found:\n",
      "40366.41458300494\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grid_params = [{'max_depth':[4,6,8,10,12],'min_samples_leaf':[4,6,8,10,12,14],'min_samples_split':[6,8,10,12,14,16]}]\n",
    "grid_search(tree.DecisionTreeRegressor(),grid_params,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error loss of our model: 36214.96\n"
     ]
    }
   ],
   "source": [
    "decisionTree = tree.DecisionTreeRegressor(max_depth=8,min_samples_leaf=10,min_samples_split=4)\n",
    "decisionTree = decisionTree.fit(x_train, y_train)\n",
    "loss = get_loss(decisionTree, x_val,y_val)\n",
    "print(\"Root Mean Squared Error loss of our model: {:.2f}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.763267622626243, 'OverallQual'),\n",
       " (0.11629030777343835, 'GrLivArea'),\n",
       " (0.04016905121931363, 'BsmtFinSF1'),\n",
       " (0.014762992463104908, 'TotalBsmtSF'),\n",
       " (0.010653167248176868, '1stFlrSF'),\n",
       " (0.008081864342630423, 'YearRemodAdd'),\n",
       " (0.006643833748505893, 'GarageArea'),\n",
       " (0.005690894354319145, 'OverallCond'),\n",
       " (0.005152151296018067, 'YearBuilt'),\n",
       " (0.0047125257695696445, 'LotArea')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance = sorted(zip(list(decisionTree.feature_importances_),feature_cols),reverse=True,key=lambda x :x[0])\n",
    "feature_importance[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pydotplus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-daeadaf6f3b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                 \u001b[0mfilled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrounded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                 special_characters=True,feature_names=feature_cols)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydotplus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_from_dot_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdot_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_png\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pydotplus' is not defined"
     ]
    }
   ],
   "source": [
    "dot_data = StringIO()\n",
    "export_graphviz(decisionTree, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True,feature_names=feature_cols)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "# Random Forest\n",
    "\n",
    "Now we have thoroughly investigated decision trees, but why stop there? We can collect a bunch of trees to form a **random forest**! A very quick reminder for what random forests are: collect a bunch of trees, and average their results. Average may be taking the majority in a classification problem, or the mean of the regression output of each tree. In order to prevent each of the trees being identical, we randomly sample portions of the data to train on and random features to use in our forest. \n",
    "\n",
    "The code is very similar and we will take advantage of sklearn's implementation of random forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error loss of our model: 31531.84\n"
     ]
    }
   ],
   "source": [
    "randomForest = RandomForestRegressor()\n",
    "randomForest = randomForest.fit(x_train, y_train)\n",
    "loss = get_loss(randomForest, x_val,y_val)\n",
    "print(\"Root Mean Squared Error loss of our model: {:.2f}\".format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! This random forest already performs far better than our other models. Can we tune some parameters as before? Now we may add the `n_estimators` parameter, which is basically how many trees we have in our ensemble. \n",
    " \n",
    "**Warning**: This may take a long time to run! Do not try to search too much at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_params = [{'max_depth':[10,12,14,16,18], 'min_samples_leaf':[1,2,4,6],'min_samples_split':[2,3,4,6,8,10],\n",
    "                     'n_estimators':[10,20,30,40,50],'random_state':[0]}]\n",
    "grid_search(RandomForestRegressor(),grid_params,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForest = RandomForestRegressor(max_depth=14,min_samples_leaf=1,\n",
    "                                     min_samples_split=6,n_estimators=40,random_state=0)\n",
    "randomForest = randomForest.fit(x_train, y_train)\n",
    "loss = get_loss(randomForest, x_val,y_val)\n",
    "print(\"Root Mean Squared Error loss of our model: {:.2f}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = sorted(zip(list(randomForest.feature_importances_),feature_cols),reverse=True,key=lambda x :x[0])\n",
    "feature_importance[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5'></a>\n",
    "# Ensemble Learning\n",
    "----\n",
    "\n",
    "Ensemble learning is one of the most widely used techniques to improved model accuracy. Many automated, programmatic frameworks for simple modeling tasks lend themselves to being ensembled. \n",
    "\n",
    "Ensembling is the process of training multiple models to do the same thing, and averaging their predictions. Ensemble models often train their models on different (overlapping) splits of the dataset. These datasets are picked randomly _**with**_ replacement. Intuitively, you can imagine training the same type of linear model and giving each model only 1 category of data to learn from. It's possible to imagine that each linear model as becoming an expert in its dataset, and can contribute unique information to other expert models. From high level perspective, ensembling is merging the expert opinions of different models.\n",
    "\n",
    "From this narrative, we can see how very different models produce a much more useful ensemble than identical models. Clearly, having 10 modes that detects numbers 0-9 is more useful to digit detection than having 10 models which all detect the number 1. This is known as independence of errors. \n",
    "\n",
    "Along this note... There are good formal **statistical** and **mathematical** reasons to do ensembling. One of the easier things to prove about ensembling is that averaging model predictions actually reduces variance on error.\n",
    "\n",
    "We just wanted to introduce you to what the math looks like, in case you're interested. The following is from the Deep Learning Book (Goodfellow 2016). Given $k$ models, with errors $\\epsilon_i$ on each example, and errors with variance $v$, the expected loss of the model decreases as more models are ensembled. Specifically, this works when the correlation between the models, $c$, is less than 1.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}\\left[\\left(\\frac{1}{k} \\sum_i \\epsilon_i\\right)^2\\right] &= \\frac{1}{k^2}\\mathbb{E}\\left[\\sum_{i}\\left(\\epsilon_i^2 + \\sum_{j\\not=i} \\epsilon_i\\epsilon_j\\right)\\right] \\\\\n",
    "&= \\frac{1}{k}v + \\frac{k-1}{k}c\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "A second good reason to use ensembling is to improve the space of representations that the aggregate (total) model can learn. Understanding this line of reasoning requires the ability to visualize a model's decision boundary. Essentially, multiple single models can overfit to the data, creating complex, abstract hyperplanes to separate between classifications.\n",
    "\n",
    "![Ensemble of overfit models](GRAPHICS/overfitboundary.png)\n",
    "\n",
    "If you're interested in reading up on this, and learning where that figure came from, check out [MLWave's explanation.](https://mlwave.com/kaggle-ensembling-guide/)\n",
    "\n",
    "### Do not worry about the math!\n",
    "\n",
    "We do not need to understand it to understand how ensembling works. We'll begin our practical walk through below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 34772.90\n",
      "Validation loss: 36802.48\n"
     ]
    }
   ],
   "source": [
    "# This is my favorite approach! Try everything lazy and grab all the low-hanging fruit!\n",
    "# Brute force everything!\n",
    "\n",
    "# Create `num_models` of models of decision trees with random max_depth\n",
    "# And train them before appending to a list of models\n",
    "# Creating LinearRegression models trained on different datasets have less correlated errors\n",
    "# TODO: Why don't you try using this function with tree.DecisionTree? This is an alternate RandomForest implementation\n",
    "# TODO: Can you extend this function to work accept any list of model functions?\n",
    "def create_models(model_func, num_models, frac_dataset, x_train):\n",
    "    \"\"\"Trains `num_models` of model_func instances on `frac_dataset` percent of the x_train.\"\"\"\n",
    "    models = []\n",
    "    for i in range(num_models):\n",
    "        num_dataset = int(frac_dataset * len(x_train))\n",
    "        indices = np.random.permutation(num_dataset)\n",
    "        x_random, y_random = x_train[indices], y_train[indices]\n",
    "        models.append(model_func().fit(x_random, y_random))\n",
    "    return models\n",
    "\n",
    "model_func = lambda : LinearRegression()\n",
    "models = create_models(model_func, 10, 0.7, x_train)\n",
    "\n",
    "def avg_model(ms):\n",
    "    \"\"\"Returns a lambda that can be used to predict over all the models.\"\"\"\n",
    "    def predict(x_input):\n",
    "        \"\"\"Ensembles model predictions using an unweighted average.\"\"\"\n",
    "        total = len(ms)\n",
    "        return sum([model.predict(x_input) for model in models]) / total\n",
    "    return predict\n",
    "\n",
    "def get_loss_from_function(predict_function, X, Y_true):\n",
    "    \"\"\"Returns square root of L2 loss (RMSE) between Y_hat and true values\n",
    "    \n",
    "    predict_function(function): model we use to predict values\n",
    "    X: numpy matrix of x values\n",
    "    Y_true: numpy matrix of true y values\n",
    "    \"\"\"\n",
    "    Y_hat = predict_function(X)\n",
    "    return np.sqrt(np.mean((Y_true-Y_hat)**2))\n",
    "\n",
    "# Now we can use the models we trained to create an averaged ensemble\n",
    "avg_model_predict = avg_model(models)\n",
    "loss = get_loss_from_function(avg_model_predict, x_train, y_train)\n",
    "print(\"Train loss: {:.2f}\".format(loss))\n",
    "loss = get_loss_from_function(avg_model_predict, x_val, y_val)\n",
    "print(\"Validation loss: {:.2f}\".format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! Clearly, the loss on this model is much better than the loss of the previous best decision tree!\n",
    "\n",
    "## Smarter Ensembling - Weighted Averages\n",
    "\n",
    "Some models are better than others, so we can train a model to learn how to weigh the predictions of the models. We can do this by saving the predictions of a bunch of models, and then learning a weighted average of them that predicts `y_train` (using a Linear Model). If using a linear model to learn the weights of the ensemble doesn't make sense - don't fret.\n",
    "\n",
    "Let the ensembled model output be $f(x)$, and the output of model $i$ be $m_i$.\n",
    "$$ f(x) = a_1*m_1(x) + a_2*m_2(x) + \\ldots + a_n*m_n(x) $$\n",
    "This is a **_linear_** transformation, which means a linear model can learn the weights $a_1, a_2, \\cdots, a_n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82004.43067306193\n",
      "82550.7214559632\n"
     ]
    }
   ],
   "source": [
    "def get_ensemble_features(models, x_input):\n",
    "    \"\"\"Returns the features for an ensemble by passing the original input through the original base models.\"\"\"\n",
    "    model_predictions = np.concatenate([model.predict(x_input) for model in models]).reshape((-1, len(models)))\n",
    "    return model_predictions\n",
    "\n",
    "ensemble_features = get_ensemble_features(models, x_train)\n",
    "\n",
    "linear_ensemble_model = Ridge().fit(ensemble_features, y_train)\n",
    "\n",
    "loss = get_loss(linear_ensemble_model, get_ensemble_features(models, x_train), y_train)\n",
    "print(loss)\n",
    "loss = get_loss(linear_ensemble_model, get_ensemble_features(models, x_val), y_val)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This clearly didn't work nearly as well as the model averaging! This is probably because the method by which Sci-Kit learn calculates least squares regression assumes very low correlation between features. It's possible that they leveraged this to produce a method which is not as good at learning a linear model over similar ensembles.\n",
    "\n",
    "Other things to try would be performing least squares regresion on the ensembles' outputs. That should probably work. In general, the practice of building models on top of other models is called **ensemble stacking.** This a very big subfield of statistical modeling that is responsible for the difference between a good model and an award-winning model. Specifically, **gradient-boosting** is a very widely used approach. See the [Additional Reading](#section) section for more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusion'></a>\n",
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='reading'></a>\n",
    "# Additional Reading\n",
    "\n",
    "This notebook we covered a lot of different modeling tools. However, we have barely scratched the surface. Even though we have not presented as much math as in previous presentations, there is a lot of theory behind these tools. \n",
    "\n",
    "The most widely used tools are variants of decision trees. One of the most popular variants of random forest modeling is called [Gradient Boosting](http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting). The framework that implements this particularly well is called [XGBoost](https://github.com/dmlc/xgboost).\n",
    "\n",
    "Sci-kit learn has a lot of ensemble methods built-in as well. Their documentation is probably some of the best out there, it's absolutely littered with examples. See the [ensemble docs](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble) for more. \n",
    "\n",
    "If you're interested in different types of models to use in your ensemble, try out Logistic Regression in the sci-kit learn library. For the full list of models, see [their docs](http://scikit-learn.org/stable/supervised_learning.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
