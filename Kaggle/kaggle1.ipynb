{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUSA CX Kaggle Capstone\n",
    "## Part 1: Introduction, Exploratory Data Analysis, and Feature Selection\n",
    "\n",
    "### Table Of Contents\n",
    "* [Introduction](#section1)\n",
    "* [Data Science Workflow](#section2)\n",
    "    1. [Understanding Our Dataset](#i)\n",
    "    2. [Data Cleaning](#ii)\n",
    "    3. [EDA](#iii)\n",
    "    4. [Modeling](#iiii)\n",
    "    5. [Model Evaluation](#v)\n",
    "* [Conclusion](#conclusion)\n",
    "* [Additional Reading](#reading)\n",
    "\n",
    "\n",
    "### Hosted by and maintained by the [Statistics Undergraduate Students Association (SUSA)](https://susa.berkeley.edu). Originally authored by [Arun Ramamurthy](mailto:contact@arun.run), [Patrick Chao](mailto:prc@berkeley.edu), & [Noah Gundotra](mailto:noah.gundotra@berkeley.edu).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "# SUSA CX Kaggle Capstone Project\n",
    "\n",
    "Welcome to the last four weeks of your semester in SUSA's Career Exploration committee! Now that you've participated in nearly a dozen workshops on Python, R, data science, and machine learning, we're going to guide you through a four-week collaborative Kaggle competition with your peers in Career Exploration. We want to give you the experience of working with real data, using real machine learning algorithms, in an educational setting. You will have to make use of your data computing skills in Python, dive into reading kernels on the Kaggle website, use visualization and feature engineering to improve your score, and maybe even pick up a few advanced deep learning models along the way. \n",
    "\n",
    "If this sounds a bit intimidating right now, do not fret! Your SUSA Mentors will be there to mentor you through the whole thing. Don't worry if our code, our approach, or any part of this process seems unnatural. It is. It takes time to learn, and it takes time to teach. We're learning as we make these tutorials, too! So without further ado, let's get cracking!\n",
    "\n",
    "## What is [Kaggle](https://www.kaggle.com/)?\n",
    "\n",
    "Kaggle is platform that hosts datasets & data science competitions. Kaggle started off as a small community of data science practitioners looking to hone their hobbying skills and meet like-minded folks, not unlike SUSA! :)\n",
    "\n",
    "\n",
    "Today they host many different types of competitions for [money from 1k to 1M](https://www.kaggle.com/c/data-science-bowl-2018), [jobs](https://www.kaggle.com/jobs), [internships](https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries) e.g. 2Sigma, and [prestige](https://www.kaggle.com/c/imagenet-object-detection-challenge). The best competitors get special badges that give their comments, contributions, and teams special status in the community. You can build your reputation by posting helpful notebooks (called **kernels** by Kaggle), like this one, on a Kaggle dataset.\n",
    "\n",
    "There's lots more to Kaggle. If you're interested some examples in getting started with Kaggle competitions, or how it works, see our [Kaggle Workshop slides!](https://github.com/ngundotra/SUSA_Kaggle_Workshop)\n",
    "\n",
    "### Kaggle Competition: Housing Prices\n",
    "\n",
    "In this project, we will be working with the [**Housing Prices**](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/) competition on the Kaggle website. The competition deals with accurately predicting the sales prices of houses in Ames, Iowa. More information on the dataset and competition will follow shortly.\n",
    "\n",
    "### Accessing the `House Prices` Dataset\n",
    "\n",
    "While you can access these datasets online at the [Kaggle webpage for this competition](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data), we have already downloaded the data for you and placed it in the `crash-course/Kaggle/DATA/house-prices/` directory. \n",
    "\n",
    "## Overview of the CX Capstone Project\n",
    "\n",
    "So where does Kaggle fit in into your CX education? So far, you've learned all sorts of skills, from the bias-variance tradeoff, to EDA, to deep learning. However, you've still only learned them in concept (with the exception of the mini-projects and workbook problems). To get a real handle on how these concepts work, you need to get some proper experience with a real-life dataset and data problem. Fortunately, Kaggle provides these datasets and problems, and so we're going to use a Kaggle competition to help you practice the skills you've learned from the CX workshop series, and hopefully bond a little bit with your fellow CX members as well! \n",
    "\n",
    "## Logistics\n",
    "\n",
    "This project is going to be tough and long, but we will be there with you through the whole thing. Additionally, you won't be working alone - we have used your input to assign you into teams of 3 - 4 members. These will be your team members for the rest of the semester, so get to know each other a little!\n",
    "\n",
    "The project will last the remaining 4 weeks of Career Ex, and one hour per each weekly CX meeting will be devoted to working on it with your team (The other hour will be devoted to guest speakers). However, that only leaves 4 hours of in-class time to work on the project. In order to complete it successfully, it is highly likely that you and your team will have to do some work outside of the weekly CX meetings. To help you stay accountable for yourself, we will be providing you with space and times to meet with SUSA Mentors to help.\n",
    "\n",
    "\n",
    "### Teams \n",
    "\n",
    "You can find the breakdown of the teams with this link:\n",
    "https://docs.google.com/document/d/1twOPXsil1ZyQ1_Rt770ubUbmdQto36OYNNhhBODukPI/edit?usp=sharing\n",
    "\n",
    "I would start by getting the Slack handle or phone number (or other contact info) of all the other members in your group so that you can easily communicate outside of the weekly meetings. It's up to you guys to determine how you are going to meet - however, we **highly encourage** all work on the project to be done **in-person together**.\n",
    "\n",
    "### Mandatory Office Hours\n",
    "\n",
    "Because this is such a large project, you and your team will surely have to work on it outside of meetings. In order to get you guys to seek help from this project, we are making it **mandatory** for you and your group to attend **two (2)** SUSA Office Hours over the next 4 weeks. This will allow questions to be answered outside of the regular meetings and will help promote collaboration with more experienced SUSA members.\n",
    "\n",
    "The schedule of SUSA office hours are below:\n",
    "https://susa.berkeley.edu/calendar#officehours-table\n",
    "\n",
    "We understand that most of you will end up going to Arun or Patrick's office hours, but we highly encourage you to go to other people's office hours as well. There are many qualified SUSA mentors who can help and this could be an opportunity for you to meet them.\n",
    "\n",
    "### SUSA Datathon\n",
    "\n",
    "One other time that we are hoping you will work on your Kaggle projects is during the SUSA Datathon. The time and place of the event is still tentative, but it will likely 4-8PM on Sunday 4/15. At the Datathon, members from many SUSA committees, notably Data Consulting, Research & Publication, and Career Exploration, will all meet up and work on their respective projects together. There will be many other SUSA members there to help you and it should be a great environment for you and your group to work. This is also another great opportunity to meet other experienced SUSA members and get a taste for other committees in the club. More details about this event will be released later.\n",
    "\n",
    "### Git\n",
    "\n",
    "Given that this is a collaborative project, you'll need to work with your team members on the same codebase simutaneously! This is fortunately simple with Git, which you learned in your very first workshop. Visit `py0` if you need a refresher, but we will be going over the steps for collaborative work here too.\n",
    "\n",
    "1. First, decide on which one of you will be hosting the forked Github repository for `crash-course`. Ideally, this would be someone with some GitHub experience and a GitHub account. If no one on your team has a GitHub account, one of you should sign up for one. For our examples, we will call this person's account name `rprincess`.\n",
    "\n",
    "2. Next, have the above person navigate to the [SUSA crash-course repository](https://github.com/SUSA-org/crash-course) and click the `Fork` button. GitHub will make a copy of the crash-course repository in your team member's account. \n",
    "\n",
    "3. Each one of you can download the `rprincess` repository to your local computer with the following command: `git clone https://github.com/rprincess/crash-course.git`. \n",
    "\n",
    "4. Feel free to work within the repository and use `git pull origin` and `git add -A && git commit -am \"My example message here\" && git push` to pull/push your local repository to the `rprincess` online repository.\n",
    "\n",
    "If you have any questions, just Slack Lucas, Noah, Patrick, or Arun and we can help you with your Git workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "# The Data Science Workflow\n",
    "\n",
    "In general, there are a few key steps to begin working with a dataset. \n",
    "\n",
    "First, we need to **understand what the dataset actually is about**, and what we are trying to do with it. Key to this stage is understanding what each row of the dataset represents, as well as what each column indicates. You can read more about this dataset by looking at the dataset itself, or reading the data dictionary in `crash-course/Kaggle/DATA/house-prices/data_description.txt`.\n",
    "\n",
    "Second, before we can even do exploratory data analysis, we need to **clean our dataset**. It is very helpful to identify the size of the dataset, so we know how many samples we have. We need to determine a consistent method of dealing with missing values, such as setting them to a value, removing the feature entirely, interpolating values, etc. Other crucial steps are separating into training and validation, as well as creating elementary data plots.\n",
    "\n",
    "Third, we have **exploratory data analysis (EDA)**. The point of this phase is to inspect & visualize key relationships, trends, outliers, and issues with our data. By conducting EDA, we get a better sense of the underlying structure of our data, and which features are most important. Especially since we have 81 features, we would like to select the features that are most important to our analysis before we begin modeling `SalePrice`. Once we have an idea about which features to use and how they relate to one another, our modeling stage will be more informed and robust. \n",
    "\n",
    "Fourth, we have the **modeling phase**, consisting of **model selection** and **model training**. Here, we select a predictive model to train on our features, and then actually train the model! In this first week, we will be using linear regression as a first-pass. In later weeks of the SUSA CX Kaggle Capstone project, we will be using more advanced models like random forest and neural networks. Depending on the model we are using, it is important to verify the model's assumptions before fully pledging to that model. Additionally, we may use **validation** in this stage to select certain hyperparameters for our model selection.\n",
    "\n",
    "Finally, we have the **model evaluation phase**. Here, we compute a metric for our model's performance, usually by  summing the squared errors of the model's predictions on the test set. This stage allows us to effectively compare various data cleaning and modeling selection decisions, by giving us a single comparable value for performance across our potential models.\n",
    "\n",
    "<a id='i'></a>\n",
    "## I. Understanding our Dataset \n",
    "Our dataset is about houses in Iowa! According to the Kaggle webpage, the competition is as follows:\n",
    "\n",
    "> Ask a home buyer to describe their dream house, and they probably won’t begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition’s dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n",
    "With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\n",
    "\n",
    "More explicitly, our dataset has 81 columns, or features: \n",
    "1. `SalePrice`, our response variable $Y$ that we are trying to predict accurately and precisely \n",
    "2. `Id`, a simple identification variable\n",
    "3. 79 explanatory variables $X_k$ that we can use to predict `SalePrice`. Some of the variables are categorical, and others are continuous quantitative. \n",
    "\n",
    "The goal of the next four weeks to to create a model that trains on (some of) the 79 explanators from the training set to predict `SalePrice` well in the test set. \n",
    "\n",
    "How will we know which explanators to use? We can start with some intuition and research into what each column represents by reading the data dictionary in `crash-course/Kaggle/DATA/house-prices/data_description.txt`. \n",
    "\n",
    "> Please take a moment to read over this dictionary, as you will need to have a keen sense of these features for the weeks ahead. Can you come up with five features you suspect will be important in determining the `SalePrice`?\n",
    "\n",
    "Now that we've talked a bit about this dataset, let's actually take a look at it. The first step is to load in the data! We will store this in a `pandas` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>RL</td>\n",
       "      <td>85.0</td>\n",
       "      <td>14115</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>Shed</td>\n",
       "      <td>700</td>\n",
       "      <td>10</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>143000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>75.0</td>\n",
       "      <td>10084</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>307000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10382</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shed</td>\n",
       "      <td>350</td>\n",
       "      <td>11</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>50</td>\n",
       "      <td>RM</td>\n",
       "      <td>51.0</td>\n",
       "      <td>6120</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>129900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>190</td>\n",
       "      <td>RL</td>\n",
       "      <td>50.0</td>\n",
       "      <td>7420</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>118000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "5   6          50       RL         85.0    14115   Pave   NaN      IR1   \n",
       "6   7          20       RL         75.0    10084   Pave   NaN      Reg   \n",
       "7   8          60       RL          NaN    10382   Pave   NaN      IR1   \n",
       "8   9          50       RM         51.0     6120   Pave   NaN      Reg   \n",
       "9  10         190       RL         50.0     7420   Pave   NaN      Reg   \n",
       "\n",
       "  LandContour Utilities    ...     PoolArea PoolQC  Fence MiscFeature MiscVal  \\\n",
       "0         Lvl    AllPub    ...            0    NaN    NaN         NaN       0   \n",
       "1         Lvl    AllPub    ...            0    NaN    NaN         NaN       0   \n",
       "2         Lvl    AllPub    ...            0    NaN    NaN         NaN       0   \n",
       "3         Lvl    AllPub    ...            0    NaN    NaN         NaN       0   \n",
       "4         Lvl    AllPub    ...            0    NaN    NaN         NaN       0   \n",
       "5         Lvl    AllPub    ...            0    NaN  MnPrv        Shed     700   \n",
       "6         Lvl    AllPub    ...            0    NaN    NaN         NaN       0   \n",
       "7         Lvl    AllPub    ...            0    NaN    NaN        Shed     350   \n",
       "8         Lvl    AllPub    ...            0    NaN    NaN         NaN       0   \n",
       "9         Lvl    AllPub    ...            0    NaN    NaN         NaN       0   \n",
       "\n",
       "  MoSold YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0      2   2008        WD         Normal     208500  \n",
       "1      5   2007        WD         Normal     181500  \n",
       "2      9   2008        WD         Normal     223500  \n",
       "3      2   2006        WD        Abnorml     140000  \n",
       "4     12   2008        WD         Normal     250000  \n",
       "5     10   2009        WD         Normal     143000  \n",
       "6      8   2007        WD         Normal     307000  \n",
       "7     11   2009        WD         Normal     200000  \n",
       "8      4   2008        WD        Abnorml     129900  \n",
       "9      1   2008        WD         Normal     118000  \n",
       "\n",
       "[10 rows x 81 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('DATA/house-prices/train.csv')\n",
    "test = pd.read_csv('DATA/house-prices/test.csv')\n",
    "\n",
    "#Let's see what the training dataframe looks like!\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions for Understanding:\n",
    "> 1. How many features (columns) do we have? How many entries (rows)?\n",
    "> 2. What does a single row represent in our dataset?\n",
    "> 3. List 3 issues/questions that you see from the dataset. Some ideas to get started: What does `LotShape` represent? Is it a good thing that we have so many features? How does a large number of features affect our modeling approach?\n",
    "> 4. What are some important things we should do for data cleaning and exploration? Some ideas to get started: are all the values in `LotFrontage` numeric? What about `Alley`? How should we fix missing/`NA` values that appear sporadically in some columns? What about columns that are almost entirely full of `NA` values? Some columns are qualitative strings, whereas others are qualitative numerics - how might this affect our cleaning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ii'></a>\n",
    "# II. Data Cleaning\n",
    "First, let's see how big our dataset looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size (1460, 81)\n",
      "Test size (1459, 80)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training size\",train.shape)\n",
    "print(\"Test size\",test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that we have $1460$ datapoints in the training set, and we would like to predict on $1459$ samples. There are $80$ total features, and one response variable we would like to predict. However, not all these 'features' are actually useful, such as `Id`. Thus we need to understand what the actual variables mean. A huge part in data science is actually understanding the variables. Of course it is possible to throw the data into some machine learning model and have it spit out predictions, but without actually understanding what data you are dealing with and how to feed the data into the model, your model is worthless.\n",
    "\n",
    "Spend a good deal of time reading over the data dictionary. It is located in  \n",
    "`crash-course/Kaggle/DATA/house-prices/data_description.txt`\n",
    "\n",
    "## Questions for Understanding:\n",
    "> 1. There are many categorical variables. What are some possibilities to deal with these variables? (Hint: [one hot encoding](https://www.kaggle.com/dansbecker/using-categorical-data-with-one-hot-encoding))\n",
    "> 2. Are there any categorical variables that we can convert to numerical/quantitative variables as well? How might we do that?\n",
    "> 3. Are there any variables that are just irrelevant and we can ignore?\n",
    "\n",
    "One helpful method is looking at the unique values of a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['PConc', 'CBlock', 'BrkTil', 'Wood', 'Slab', 'Stone'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can view the unique values of a given feature\n",
    "train['Foundation'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with NA values\n",
    "In almost all datasets, we will have NA values. These can be a pain to deal with, as there are many viable choices of what to do. First, it is good to see what columns have NA values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                  0\n",
       "MSSubClass          0\n",
       "MSZoning            0\n",
       "LotFrontage       259\n",
       "LotArea             0\n",
       "Street              0\n",
       "Alley            1369\n",
       "LotShape            0\n",
       "LandContour         0\n",
       "Utilities           0\n",
       "LotConfig           0\n",
       "LandSlope           0\n",
       "Neighborhood        0\n",
       "Condition1          0\n",
       "Condition2          0\n",
       "BldgType            0\n",
       "HouseStyle          0\n",
       "OverallQual         0\n",
       "OverallCond         0\n",
       "YearBuilt           0\n",
       "YearRemodAdd        0\n",
       "RoofStyle           0\n",
       "RoofMatl            0\n",
       "Exterior1st         0\n",
       "Exterior2nd         0\n",
       "MasVnrType          8\n",
       "MasVnrArea          8\n",
       "ExterQual           0\n",
       "ExterCond           0\n",
       "Foundation          0\n",
       "                 ... \n",
       "BedroomAbvGr        0\n",
       "KitchenAbvGr        0\n",
       "KitchenQual         0\n",
       "TotRmsAbvGrd        0\n",
       "Functional          0\n",
       "Fireplaces          0\n",
       "FireplaceQu       690\n",
       "GarageType         81\n",
       "GarageYrBlt        81\n",
       "GarageFinish       81\n",
       "GarageCars          0\n",
       "GarageArea          0\n",
       "GarageQual         81\n",
       "GarageCond         81\n",
       "PavedDrive          0\n",
       "WoodDeckSF          0\n",
       "OpenPorchSF         0\n",
       "EnclosedPorch       0\n",
       "3SsnPorch           0\n",
       "ScreenPorch         0\n",
       "PoolArea            0\n",
       "PoolQC           1453\n",
       "Fence            1179\n",
       "MiscFeature      1406\n",
       "MiscVal             0\n",
       "MoSold              0\n",
       "YrSold              0\n",
       "SaleType            0\n",
       "SaleCondition       0\n",
       "SalePrice           0\n",
       "Length: 81, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sum the number of NA's in each column\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that many of the features have a great deal of NAs. However, this is not necessarily the case. The astute reader will notice that some variables like `Alley`, `PoolQC`, and `Fence`, have NA has an actual value. For example for `Pool Quality`, we have these possibilities.\n",
    "\n",
    "PoolQC: Pool quality\n",
    "\t\t\n",
    "       Ex\tExcellent\n",
    "       Gd\tGood\n",
    "       TA\tAverage/Typical\n",
    "       Fa\tFair\n",
    "       NA\tNo Pool\n",
    "So having NA values is not the usual not available, it can actually be a legitimate value! We need to parse through the data dictionary to see when NA's are actually significant, and when they actually mean NA.\n",
    "\n",
    "## Questions for Understanding:\n",
    "> 1. Go through the data dictionary and find all features that have NA as legitimate values.\n",
    "> 2. How can we refactor the variables in question 1 appropriately?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features that we found with NA's are:\n",
    "`Alley`, `BsmtQual`, `BsmtCond`, `BsmtExposure`, `BsmtFinType1`, `BsmtFinType2`, `FireplaceQu`, `GarageType`, `GarageFinish`, `GarageQual`, `GarageCond`, `PoolQC`, `Fence`, `Misc Feature`.\n",
    "\n",
    "Alley: NA for no alley access  \n",
    "`BsmtQual`, `BsmtCond`, `BsmtExposure`, `BsmtFinType1`, `BsmtFinType2`: NA for no basement  \n",
    "`FireplaceQu`: NA for no fireplace  \n",
    "`GarageType`, `GarageFinish`, `GarageQual`, `GarageCond`: NA for no garage  \n",
    "`PoolQC`: NA for no pool  \n",
    "`Fence`: NA for no fence  \n",
    "`Misc Feature`: NA for no other miscellanous features (i.e. elevator, 2nd garage, shed, tennis Court, other) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning Functions\n",
    "We provide some functions to help with data cleaning and preprocessing. One is used to create one hot encodings of various features, and the latter is for converting a categorical feature into values.\n",
    "\n",
    "### One Hot Encoding for Unordered Categorical Variables\n",
    "Consider we have a feature `committee` for a data frame of SUSA members. Let's say for sake of simplicity there are four committees, `CX`, `DC`, `RP`, and `WD`. There is no inherent ordering to the features, but each member is only in one committee. Thus we can replace this categorical variable with a single boolean for `committee`. If a member is a part of `CX`, then they will have a $1$ for `DC` and $0$ for all other variables. We have written this function for you in `oneHotFeature`.\n",
    "\n",
    "### Ordered Categorical Variables\n",
    "Consider we have a feature `attendance` for a data frame of SUSA members. Let's say there are $3$ possible values, `Good`, `Ok`, `Poor`. These inherently have an ordering, `Good` is better than `Okay` which is better than `Poor`. We can assign a numerical value to these, such as $0$ for `Poor`, $1$ for `Ok`, and $2$ for `Good`. We have written this function for you in `categoricalToQuantitative`.\n",
    "\n",
    "\n",
    "Please read over the following functions and read the comments carefully! They describe in detail what the functions do, and are **very** crucial to the data cleaning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning Functions\n",
    "\n",
    "def oneHotFeature(df, features, withNA=True):\n",
    "    \"\"\"\n",
    "    This function is for unordered categorical features \n",
    "\n",
    "    This function takes in input as: \n",
    "    Data frame 'df'\n",
    "    List of features to one hot encode 'features'\n",
    "    Boolean for how to deal with NA's 'withNA'\n",
    "\n",
    "    This function creates a set of output features from a categorical feature\n",
    "    The output features are one hot encodings with names featureName_{value}\n",
    "    Returns a new data frame (does not modify original dataframe) with appended features\n",
    "    Usually the NA values are also considered\n",
    "    Will add a column featureName_none one hot encoding of NA values\n",
    "    If the boolean withNA is false, it will not consider NA values\n",
    "\n",
    "    If a feature is not found, it will ignore that feature and attempt to one hot the other features\n",
    "\n",
    "    Code from Numpy and Pandas SUSA guide\n",
    "    crash-course/Python/Numpy and Pandas.ipynb\n",
    "    \"\"\"\n",
    "    # Copy over data\n",
    "    newDf = df.copy()\n",
    "    for feature in features:\n",
    "        try:\n",
    "            if withNA:\n",
    "                newDf[feature] = newDf[feature].fillna('none')\n",
    "            col_onehot = pd.get_dummies(newDf[feature], prefix=feature) \n",
    "            newDf = newDf.drop(feature, axis=1)\n",
    "            newDf = newDf.join(col_onehot)\n",
    "        except:\n",
    "            print(\"No such feature\", feature, \"found in the dataframe when trying to one-hot encode\")\n",
    "    return newDf\n",
    "\n",
    "\n",
    "\n",
    "def categoricalToQuantitative(df,feature, mapping,assumeInOrder = False):\n",
    "    \"\"\"\n",
    "    This function is for ordered categorical features \n",
    "\n",
    "    This function takes in input as: \n",
    "    Data frame 'df'\n",
    "    A single categorical feature to to be mapped 'feature'\n",
    "    A mapping dictionary 'mapping'\n",
    "\n",
    "    This function creates takes a dataframe and categorical feature, and maps the categorical values\n",
    "    using the dictionary mapping\n",
    "    Returns a new data frame (does not modify original dataframe) with modified values for the feature column\n",
    "    For mappings with NA values, use 'NA' in the dictionary, this function properly deals with them\n",
    "\n",
    "    If the mapping is just from 0 to n-1 for n values\n",
    "    Then set default to True, and mapping is instead a list of ordering from worst to best\n",
    "\n",
    "    If a feature is not found, the function will fail\n",
    "    \"\"\"\n",
    "    newDf = df.copy()\n",
    "    try:\n",
    "        currFeature = df[feature]\n",
    "    except:\n",
    "        print(\"No such feature\", feature, \"found in the dataframe when trying to map\")\n",
    "        return newDf\n",
    "    # Check if input mapping and all unique values for feature are equal\n",
    "    newDf[feature] = newDf[feature].fillna('nan')\n",
    "    if assumeInOrder:\n",
    "        keys = mapping\n",
    "    else:\n",
    "        keys = mapping.keys()\n",
    "    uniqueSet = set(newDf[feature].unique())\n",
    "    keySet = set(keys)\n",
    "    if 'NA' in keySet:\n",
    "        keySet.add('nan')\n",
    "        keySet.remove('NA')\n",
    "    diff = uniqueSet.difference(keySet)\n",
    "    diff2 = keySet.difference(uniqueSet)\n",
    "    if len(diff) != 0:\n",
    "        print(\"Missing value(s)\",diff,\"in mapping\",feature,\"unable to map all values\")\n",
    "        return newDf\n",
    "    if len(diff2) != 0:\n",
    "        print(\"Warning: no such values\",diff2,\"in feature\",feature,\"(they may not appear)\")\n",
    "    # Create mapping\n",
    "    if assumeInOrder:\n",
    "        newMapping ={}\n",
    "        for i in range(len(mapping)):\n",
    "            if mapping[i] == 'NA':\n",
    "                newMapping['nan'] = i \n",
    "            else:\n",
    "                newMapping[mapping[i]] = i\n",
    "    else:    \n",
    "        newMapping = mapping.copy()\n",
    "        if 'NA' in newMapping.keys():\n",
    "            newMapping['nan'] = newMapping['NA']\n",
    "    newDf[feature] = newDf[feature].apply(lambda feat: newMapping[feat] if feat in newMapping.keys() else feat)\n",
    "    return newDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the first function `oneHotFeature` first. Consider the `Heating` feature. There are $5$ possible values, Brick & Tile, Cinder Block, Poured Concrete, Slab, Stone and Wood. There isn't an inherent ordering to these, where one is better than the other. The best way to go about preprocessing the data is through one hot encoding the data. We remove the `Foundation` feature and replace it with $5$ separate boolean features, one for each of the possible values.\n",
    "\n",
    "The dataframe is shown below. Keep in mind that the original dataframe, `train`, is not modified in this function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "      <th>Foundation_BrkTil</th>\n",
       "      <th>Foundation_CBlock</th>\n",
       "      <th>Foundation_PConc</th>\n",
       "      <th>Foundation_Slab</th>\n",
       "      <th>Foundation_Stone</th>\n",
       "      <th>Foundation_Wood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>50</td>\n",
       "      <td>RL</td>\n",
       "      <td>85.0</td>\n",
       "      <td>14115</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>143000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>75.0</td>\n",
       "      <td>10084</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>307000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10382</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>200000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>50</td>\n",
       "      <td>RM</td>\n",
       "      <td>51.0</td>\n",
       "      <td>6120</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>129900</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>190</td>\n",
       "      <td>RL</td>\n",
       "      <td>50.0</td>\n",
       "      <td>7420</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>118000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "5   6          50       RL         85.0    14115   Pave   NaN      IR1   \n",
       "6   7          20       RL         75.0    10084   Pave   NaN      Reg   \n",
       "7   8          60       RL          NaN    10382   Pave   NaN      IR1   \n",
       "8   9          50       RM         51.0     6120   Pave   NaN      Reg   \n",
       "9  10         190       RL         50.0     7420   Pave   NaN      Reg   \n",
       "\n",
       "  LandContour Utilities       ...        YrSold SaleType SaleCondition  \\\n",
       "0         Lvl    AllPub       ...          2008       WD        Normal   \n",
       "1         Lvl    AllPub       ...          2007       WD        Normal   \n",
       "2         Lvl    AllPub       ...          2008       WD        Normal   \n",
       "3         Lvl    AllPub       ...          2006       WD       Abnorml   \n",
       "4         Lvl    AllPub       ...          2008       WD        Normal   \n",
       "5         Lvl    AllPub       ...          2009       WD        Normal   \n",
       "6         Lvl    AllPub       ...          2007       WD        Normal   \n",
       "7         Lvl    AllPub       ...          2009       WD        Normal   \n",
       "8         Lvl    AllPub       ...          2008       WD       Abnorml   \n",
       "9         Lvl    AllPub       ...          2008       WD        Normal   \n",
       "\n",
       "  SalePrice Foundation_BrkTil Foundation_CBlock Foundation_PConc  \\\n",
       "0    208500                 0                 0                1   \n",
       "1    181500                 0                 1                0   \n",
       "2    223500                 0                 0                1   \n",
       "3    140000                 1                 0                0   \n",
       "4    250000                 0                 0                1   \n",
       "5    143000                 0                 0                0   \n",
       "6    307000                 0                 0                1   \n",
       "7    200000                 0                 1                0   \n",
       "8    129900                 1                 0                0   \n",
       "9    118000                 1                 0                0   \n",
       "\n",
       "   Foundation_Slab  Foundation_Stone  Foundation_Wood  \n",
       "0                0                 0                0  \n",
       "1                0                 0                0  \n",
       "2                0                 0                0  \n",
       "3                0                 0                0  \n",
       "4                0                 0                0  \n",
       "5                0                 0                1  \n",
       "6                0                 0                0  \n",
       "7                0                 0                0  \n",
       "8                0                 0                0  \n",
       "9                0                 0                0  \n",
       "\n",
       "[10 rows x 86 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneHotFeature(train,['Foundation']).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     PConc\n",
       "1    CBlock\n",
       "2     PConc\n",
       "3    BrkTil\n",
       "4     PConc\n",
       "5      Wood\n",
       "6     PConc\n",
       "7    CBlock\n",
       "8    BrkTil\n",
       "9    BrkTil\n",
       "Name: Foundation, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train is not modified\n",
    "train['Foundation'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we may try this with the other function `categoricalToQuantitative`, mapping a categorical data where there is an quantitative correspondence. Consider the feature `FireplaceQu`. There are $6$ possible values, `Ex`, `Gd`, `TA`, `Fa`, `Po`, `NA`, corresponding to Excellent, Good, Average, Fair, Poor, and No fireplace. These could be mapped to the values from $0$ to $5$, in order of quality. Thus we can make a dictionary of values, where `Ex` corresponds to $5$, `Gd` corresponds to $4$, ..., `NA` corresponds to zero. This is written out below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    NaN\n",
      "1     TA\n",
      "2     TA\n",
      "3     Gd\n",
      "4     TA\n",
      "5    NaN\n",
      "6     Gd\n",
      "7     TA\n",
      "8     TA\n",
      "9     TA\n",
      "Name: FireplaceQu, dtype: object\n",
      "0    0\n",
      "1    3\n",
      "2    3\n",
      "3    4\n",
      "4    3\n",
      "5    0\n",
      "6    4\n",
      "7    3\n",
      "8    3\n",
      "9    3\n",
      "Name: FireplaceQu, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "mapping = {}\n",
    "mapping['NA'] = 0\n",
    "mapping['Po'] = 1\n",
    "mapping['Fa'] = 2\n",
    "mapping['TA'] = 3\n",
    "mapping['Gd'] = 4\n",
    "mapping['Ex'] = 5\n",
    "\n",
    "print(train['FireplaceQu'].head(10))\n",
    "print(categoricalToQuantitative(train,'FireplaceQu',mapping)['FireplaceQu'].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it is a bit annoying to type out the entire dictionary like that if we would like to order from $0$ to $5$. Another variable in `categoricalToQuantitative` is `default`. If `assumeInOrder` is true, then mapping is instead a list of variables rather than a dictionary, and we can just pass in the order of values from worst to best. This maps from $0$ to $n-1$ when there are $n$ possible values. This is more convenient, but it may be better at times to be able to customize how you want to map the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0\n",
      "1    3\n",
      "2    3\n",
      "3    4\n",
      "4    3\n",
      "5    0\n",
      "6    4\n",
      "7    3\n",
      "8    3\n",
      "9    3\n",
      "Name: FireplaceQu, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "mapping = ['NA','Po','Fa','TA','Gd','Ex']\n",
    "#Notice these mappings are the same\n",
    "print(categoricalToQuantitative(train,'FireplaceQu',mapping,assumeInOrder=True)['FireplaceQu'].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to remove a feature, use the code like the example shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`example_df.drop(bad_feature, axis=1)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the above two functions to clean the dataset! This may take a while, but doing a good job should take a while. Decide what variables are not worth keeping, decide what categorical features need to be changed, and how they should be changed. Consider how to deal with NA values, and keep all these commands together. We would recommend to save the final cleaned file as a csv so that you may easily reopen and send it, and also keep all the commands together neatly in the code block below.\n",
    "\n",
    "Some recommendations:\n",
    "> 1. Convert all features into quantitative values\n",
    "> 2. While cleaning, keep in mind some features that you feel are very helpful.\n",
    "> 3. Remove features that do not seem important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>...</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "      <th>Foundation_BrkTil</th>\n",
       "      <th>Foundation_CBlock</th>\n",
       "      <th>Foundation_PConc</th>\n",
       "      <th>Foundation_Slab</th>\n",
       "      <th>Foundation_Stone</th>\n",
       "      <th>Foundation_Wood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>...</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>...</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50</td>\n",
       "      <td>RL</td>\n",
       "      <td>85.0</td>\n",
       "      <td>14115</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>143000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>75.0</td>\n",
       "      <td>10084</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>307000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10382</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>...</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>200000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>50</td>\n",
       "      <td>RM</td>\n",
       "      <td>51.0</td>\n",
       "      <td>6120</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>129900</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>190</td>\n",
       "      <td>RL</td>\n",
       "      <td>50.0</td>\n",
       "      <td>7420</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>...</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>118000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "5          50       RL         85.0    14115   Pave   NaN      IR1   \n",
       "6          20       RL         75.0    10084   Pave   NaN      Reg   \n",
       "7          60       RL          NaN    10382   Pave   NaN      IR1   \n",
       "8          50       RM         51.0     6120   Pave   NaN      Reg   \n",
       "9         190       RL         50.0     7420   Pave   NaN      Reg   \n",
       "\n",
       "  LandContour Utilities LotConfig       ...        YrSold SaleType  \\\n",
       "0         Lvl    AllPub    Inside       ...          2008       WD   \n",
       "1         Lvl    AllPub       FR2       ...          2007       WD   \n",
       "2         Lvl    AllPub    Inside       ...          2008       WD   \n",
       "3         Lvl    AllPub    Corner       ...          2006       WD   \n",
       "4         Lvl    AllPub       FR2       ...          2008       WD   \n",
       "5         Lvl    AllPub    Inside       ...          2009       WD   \n",
       "6         Lvl    AllPub    Inside       ...          2007       WD   \n",
       "7         Lvl    AllPub    Corner       ...          2009       WD   \n",
       "8         Lvl    AllPub    Inside       ...          2008       WD   \n",
       "9         Lvl    AllPub    Corner       ...          2008       WD   \n",
       "\n",
       "  SaleCondition SalePrice Foundation_BrkTil Foundation_CBlock  \\\n",
       "0        Normal    208500                 0                 0   \n",
       "1        Normal    181500                 0                 1   \n",
       "2        Normal    223500                 0                 0   \n",
       "3       Abnorml    140000                 1                 0   \n",
       "4        Normal    250000                 0                 0   \n",
       "5        Normal    143000                 0                 0   \n",
       "6        Normal    307000                 0                 0   \n",
       "7        Normal    200000                 0                 1   \n",
       "8       Abnorml    129900                 1                 0   \n",
       "9        Normal    118000                 1                 0   \n",
       "\n",
       "   Foundation_PConc  Foundation_Slab  Foundation_Stone  Foundation_Wood  \n",
       "0                 1                0                 0                0  \n",
       "1                 0                0                 0                0  \n",
       "2                 1                0                 0                0  \n",
       "3                 0                0                 0                0  \n",
       "4                 1                0                 0                0  \n",
       "5                 0                0                 0                1  \n",
       "6                 1                0                 0                0  \n",
       "7                 0                0                 0                0  \n",
       "8                 0                0                 0                0  \n",
       "9                 0                0                 0                0  \n",
       "\n",
       "[10 rows x 85 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DATA CLEANING\n",
    "# To start off data cleaning\n",
    "clean = train.copy()\n",
    "\n",
    "clean = categoricalToQuantitative(clean,'FireplaceQu',['NA','Po','Fa','TA','Gd','Ex'],assumeInOrder=True)\n",
    "clean = oneHotFeature(clean,['Foundation'])\n",
    "\n",
    "# ====================== DO NOT INCLUDE IN FINAL WORKBOOK ========================\n",
    "# ================================================================================\n",
    "\n",
    "    \n",
    "# ================================================================================\n",
    "# ================================================================================\n",
    "\n",
    "# You will be selecting your important features explicitly later. \n",
    "# But is there anything you want to get rid of explicitly now?\n",
    "unimportantFeatures = ['Id'] ## ADD MORE FEATURES?\n",
    "for feat in unimportantFeatures:\n",
    "    clean = clean.drop(feat, axis=1)\n",
    "\n",
    "## What does \n",
    "clean.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save the dataframe as csv, run this line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv\n",
    "clean.to_csv('DATA/house-prices/train_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='iii'></a>\n",
    "# III. Exploratory Data Analysis\n",
    "\n",
    "Now that we've cleaned up our data, at this point you would usually do a careful, thorough investigation into the patterns, outliers, and relationships between your features and the response variable, and the features with one another. This is often the most creative - and hardest - part of the Data Science Workflow. While you will be conducting more EDA on your cleaned dataset in future weeks, for now we're just going to go over some essentials of EDA and a couple of visualizations for you to interpret. \n",
    "\n",
    "There are a few things you should watch for when you conduct EDA. You should always check for outliers, to get a better idea of where your model may go wrong. We're going to skip this step for now, but you should check for outliers in this dataset when you work with your team or over SUSA Office Hours! EDA also helps give you a more explicit understanding of your data, and allows you to make educated decisions in your modeling design.\n",
    "\n",
    "For example, to better guess which features we should include in our model, there are a few guidelines that EDA can shed light upon. First, it is generally a bad idea to use too many features that are correlated (a.k.a high colinearity). Secondly, we can use EDA to see which features correlate with our response variable, `SalePrice` - these features will likely be useful in forming our model. \n",
    "\n",
    "## Avoiding Colinearity\n",
    "\n",
    "**Colinear** features are variables that can be exactly represented with a linear relationship. It is a statistical fact that you cannot make a linear model for features that are in any way colinear. Even if two features are not colinear, if they are highly correlated or nearly colinear, you want to avoid using both features if possible. This is because our model will get confused about which feature affects our response variable, since both variables affect each other too. \n",
    "\n",
    "You can check for high correlation with a **correlation plot**, which uses color to indicate which pairs of features are correlated. Below is a correlation plot of features with an absolute correlation of .7 or above. \n",
    "\n",
    "![](GRAPHICS/corr_features.png)\n",
    "\n",
    "> Which pairs of features are highly correlated? When you select your features, make sure you don't select pairs of features with a pinkish or greenish correlation square. \n",
    "\n",
    "One thing to keep aware of is that correlation only indicates what you might intuit it to mean for linear relationships. You can use a **pairs plot** to see the pairwise relationships between features in your data. \n",
    "\n",
    "![](GRAPHICS/corr_pairs.png)\n",
    "\n",
    "> Which of the correlated features above show a linear trend in the related pair plot?\n",
    "\n",
    "## Features Correlated with the Response Variable\n",
    "\n",
    "To inform which features will be important in predicting `SalePrice`, it would be useful to see which of the 79 features are correlated with `SalePrice`. Below is a matrix of plots - each plot is a simple linear regression between an individual numerical feature with `SalePrice`. \n",
    "\n",
    "![](GRAPHICS/y_pairs.png)\n",
    "\n",
    "> Which of these features are associated with a high `SalePrice`? How might this inform your model selection?\n",
    "\n",
    "There is definitely a lot more to be said on how EDA can inform your modeling process and verify your model assumptions. For more information, check out the `r3` workshop or stay tuned for later weeks' expansionary material on EDA!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='iiii'></a>\n",
    "# IV. Modeling\n",
    "\n",
    "An important part to any good statisticians toolkit.\n",
    "\n",
    "> ***What I cannot create, I cannot understand.***\n",
    ">\n",
    ">  \\- Richard Feynman (Honorary Statistician)\n",
    "\n",
    "## Feature Selection\n",
    "\n",
    "Modeling begins with a guess. To create a model for data, you must guess the relevant features necessary to recreate the data distribution. Even among expert statisticians, this is regarded as a hard skill, bordering on art. Oftentimes, industry experts will provide insight as to what the right features to select are, and stasticians (like ourselves) will have to create a model from those features.\n",
    "\n",
    "There are lots of mathematical principles to guide your feature selection. So we'll begin with a theorem.\n",
    "> **Theorem 1.1**\n",
    ">\n",
    "> -- just kidding we're not that evil\n",
    "\n",
    "But seriously, there are lots of principles on how to do this correctly. But for now, do whatever feels intuitive. Explore. Create. Be inefficient. Only through walking can you learn to run. \n",
    "\n",
    "## A First Approach to Machine Learning: Linear Regression\n",
    "\n",
    "Linear regression is the most important tool in a modeler's toolkit. It's the basis for which almost all other modeling techniques arise. Essentially, it's a way to model a variable out a weighted combination of other random variables in the data.\n",
    "\n",
    "$$ \\hat{Y} = aA + bB +cC + \\ldots $$\n",
    "where **a, b, c** are (scalar) weight values, and **A, B, C** are features in the dataset, and **$\\hat{Y}$** is our modeled variable.\n",
    "\n",
    "Mathematicians have derivatives, functions, and domains.\n",
    "\n",
    "Statisticians have linear regression, data, and raw untamed IQ.\n",
    "\n",
    "Okay so the gameplan is to give an example of how to **a) select features** and how to **b) create a model**. Specifically, we ask you to model the `SalePrice` column of the dataframe from any combination of features you choose.\n",
    "\n",
    "For our linear model, we will be using the linear regression model from scikit learn. The docs are provided [here](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).\n",
    "\n",
    "#### [Meta Tips]\n",
    "Good ways to approach things you are not familiar with: here's my steps for this\n",
    "1. What is a linear model?\n",
    "2. What inputs does it take? What ouput does it give?\n",
    "3. How do I get inputs from the Pandas dataframe into a format that works with the Linear Regression model?\n",
    "4. Did I run the model correctly?\n",
    "5. How can I tell?\n",
    "6. What are other inputs I can try?\n",
    "\n",
    "... etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    208500\n",
       "1    181500\n",
       "2    223500\n",
       "3    140000\n",
       "4    250000\n",
       "Name: SalePrice, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's what our 'SalePrice' column looks like\n",
    "clean['SalePrice'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8450  9600 11250  9550 14260]\n",
      "[3 3 3 3 3]\n",
      "[5 8 5 5 5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# dataframe['column'].values returns the numpy array of that column\n",
    "# to check the type of each array, you can use `.dtype` on any numpy array\n",
    "\n",
    "print(clean['LotArea'].values[:5])\n",
    "print(clean['Utilities'].values[:5])\n",
    "print(clean['OverallCond'].values[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>ExterQual</th>\n",
       "      <th>ExterCond</th>\n",
       "      <th>BsmtQual</th>\n",
       "      <th>BsmtCond</th>\n",
       "      <th>BsmtExposure</th>\n",
       "      <th>BsmtFinType1</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinType2</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>706</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>978</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>162.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>486</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>216</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>350.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>655</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MasVnrArea  ExterQual  ExterCond  BsmtQual  BsmtCond  BsmtExposure  \\\n",
       "0       196.0          3          2         4         3             1   \n",
       "1         0.0          2          2         4         3             4   \n",
       "2       162.0          3          2         4         3             2   \n",
       "3         0.0          2          2         3         4             1   \n",
       "4       350.0          3          2         4         3             3   \n",
       "\n",
       "   BsmtFinType1  BsmtFinSF1  BsmtFinType2  BsmtFinSF2  \n",
       "0             6         706             1           0  \n",
       "1             5         978             1           0  \n",
       "2             6         486             1           0  \n",
       "3             5         216             1           0  \n",
       "4             6         655             1           0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean[clean.columns[10:20]].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.960e+02, 0.000e+00, 1.620e+02, 3.500e+02, 1.860e+02, 2.400e+02,\n",
       "       2.860e+02, 3.060e+02, 2.120e+02, 1.800e+02, 3.800e+02, 2.810e+02,\n",
       "       6.400e+02, 2.000e+02, 2.460e+02, 1.320e+02, 6.500e+02, 1.010e+02,\n",
       "       4.120e+02, 2.720e+02, 4.560e+02, 1.031e+03, 1.780e+02, 5.730e+02,\n",
       "       3.440e+02, 2.870e+02, 1.670e+02, 1.115e+03, 4.000e+01, 1.040e+02,\n",
       "       5.760e+02, 4.430e+02, 4.680e+02, 6.600e+01, 2.200e+01, 2.840e+02,\n",
       "       7.600e+01, 2.030e+02, 6.800e+01, 1.830e+02, 4.800e+01, 2.800e+01,\n",
       "       3.360e+02, 6.000e+02, 7.680e+02, 4.800e+02, 2.200e+02, 1.840e+02,\n",
       "       1.129e+03, 1.160e+02, 1.350e+02, 2.660e+02, 8.500e+01, 3.090e+02,\n",
       "       1.360e+02, 2.880e+02, 7.000e+01, 3.200e+02, 5.000e+01, 1.200e+02,\n",
       "       4.360e+02, 2.520e+02, 8.400e+01, 6.640e+02, 2.260e+02, 3.000e+02,\n",
       "       6.530e+02, 1.120e+02, 4.910e+02, 2.680e+02, 7.480e+02, 9.800e+01,\n",
       "       2.750e+02, 1.380e+02, 2.050e+02, 2.620e+02, 1.280e+02, 2.600e+02,\n",
       "       1.530e+02, 6.400e+01, 3.120e+02, 1.600e+01, 9.220e+02, 1.420e+02,\n",
       "       2.900e+02, 1.270e+02, 5.060e+02, 2.970e+02,       nan, 6.040e+02,\n",
       "       2.540e+02, 3.600e+01, 1.020e+02, 4.720e+02, 4.810e+02, 1.080e+02,\n",
       "       3.020e+02, 1.720e+02, 3.990e+02, 2.700e+02, 4.600e+01, 2.100e+02,\n",
       "       1.740e+02, 3.480e+02, 3.150e+02, 2.990e+02, 3.400e+02, 1.660e+02,\n",
       "       7.200e+01, 3.100e+01, 3.400e+01, 2.380e+02, 1.600e+03, 3.650e+02,\n",
       "       5.600e+01, 1.500e+02, 2.780e+02, 2.560e+02, 2.250e+02, 3.700e+02,\n",
       "       3.880e+02, 1.750e+02, 2.960e+02, 1.460e+02, 1.130e+02, 1.760e+02,\n",
       "       6.160e+02, 3.000e+01, 1.060e+02, 8.700e+02, 3.620e+02, 5.300e+02,\n",
       "       5.000e+02, 5.100e+02, 2.470e+02, 3.050e+02, 2.550e+02, 1.250e+02,\n",
       "       1.000e+02, 4.320e+02, 1.260e+02, 4.730e+02, 7.400e+01, 1.450e+02,\n",
       "       2.320e+02, 3.760e+02, 4.200e+01, 1.610e+02, 1.100e+02, 1.800e+01,\n",
       "       2.240e+02, 2.480e+02, 8.000e+01, 3.040e+02, 2.150e+02, 7.720e+02,\n",
       "       4.350e+02, 3.780e+02, 5.620e+02, 1.680e+02, 8.900e+01, 2.850e+02,\n",
       "       3.600e+02, 9.400e+01, 3.330e+02, 9.210e+02, 7.620e+02, 5.940e+02,\n",
       "       2.190e+02, 1.880e+02, 4.790e+02, 5.840e+02, 1.820e+02, 2.500e+02,\n",
       "       2.920e+02, 2.450e+02, 2.070e+02, 8.200e+01, 9.700e+01, 3.350e+02,\n",
       "       2.080e+02, 4.200e+02, 1.700e+02, 4.590e+02, 2.800e+02, 9.900e+01,\n",
       "       1.920e+02, 2.040e+02, 2.330e+02, 1.560e+02, 4.520e+02, 5.130e+02,\n",
       "       2.610e+02, 1.640e+02, 2.590e+02, 2.090e+02, 2.630e+02, 2.160e+02,\n",
       "       3.510e+02, 6.600e+02, 3.810e+02, 5.400e+01, 5.280e+02, 2.580e+02,\n",
       "       4.640e+02, 5.700e+01, 1.470e+02, 1.170e+03, 2.930e+02, 6.300e+02,\n",
       "       4.660e+02, 1.090e+02, 4.100e+01, 1.600e+02, 2.890e+02, 6.510e+02,\n",
       "       1.690e+02, 9.500e+01, 4.420e+02, 2.020e+02, 3.380e+02, 8.940e+02,\n",
       "       3.280e+02, 6.730e+02, 6.030e+02, 1.000e+00, 3.750e+02, 9.000e+01,\n",
       "       3.800e+01, 1.570e+02, 1.100e+01, 1.400e+02, 1.300e+02, 1.480e+02,\n",
       "       8.600e+02, 4.240e+02, 1.047e+03, 2.430e+02, 8.160e+02, 3.870e+02,\n",
       "       2.230e+02, 1.580e+02, 1.370e+02, 1.150e+02, 1.890e+02, 2.740e+02,\n",
       "       1.170e+02, 6.000e+01, 1.220e+02, 9.200e+01, 4.150e+02, 7.600e+02,\n",
       "       2.700e+01, 7.500e+01, 3.610e+02, 1.050e+02, 3.420e+02, 2.980e+02,\n",
       "       5.410e+02, 2.360e+02, 1.440e+02, 4.230e+02, 4.400e+01, 1.510e+02,\n",
       "       9.750e+02, 4.500e+02, 2.300e+02, 5.710e+02, 2.400e+01, 5.300e+01,\n",
       "       2.060e+02, 1.400e+01, 3.240e+02, 2.950e+02, 3.960e+02, 6.700e+01,\n",
       "       1.540e+02, 4.250e+02, 4.500e+01, 1.378e+03, 3.370e+02, 1.490e+02,\n",
       "       1.430e+02, 5.100e+01, 1.710e+02, 2.340e+02, 6.300e+01, 7.660e+02,\n",
       "       3.200e+01, 8.100e+01, 1.630e+02, 5.540e+02, 2.180e+02, 6.320e+02,\n",
       "       1.140e+02, 5.670e+02, 3.590e+02, 4.510e+02, 6.210e+02, 7.880e+02,\n",
       "       8.600e+01, 7.960e+02, 3.910e+02, 2.280e+02, 8.800e+01, 1.650e+02,\n",
       "       4.280e+02, 4.100e+02, 5.640e+02, 3.680e+02, 3.180e+02, 5.790e+02,\n",
       "       6.500e+01, 7.050e+02, 4.080e+02, 2.440e+02, 1.230e+02, 3.660e+02,\n",
       "       7.310e+02, 4.480e+02, 2.940e+02, 3.100e+02, 2.370e+02, 4.260e+02,\n",
       "       9.600e+01, 4.380e+02, 1.940e+02, 1.190e+02])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean['MasVnrArea'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression # There are lots of other models from this module you can try!\n",
    "\n",
    "def get_features(data, col_list, y_name):\n",
    "    \"\"\"\n",
    "    Function to return a numpy matrix of pandas dataframe features. \n",
    "    This is not a smart function - although it does drop rows with NA values. It might break. \n",
    "    \n",
    "    data(DataFrame): e.g. train, clean\n",
    "    col_list(list): list of columns to extract data from\n",
    "    y_name(string): name of the column you to treat as the y column\n",
    "    \n",
    "    Ideally returns np.array of shape (len(data), len(col_list)), and one of shape (len(data), len(col_list))\n",
    "    \"\"\"\n",
    "    \n",
    "    # keep track of numpy values\n",
    "    feature_matrix = data[col_list + [y_name]].dropna().values\n",
    "    return feature_matrix[:, :-1], feature_matrix[:, -1]\n",
    "    \n",
    "\n",
    "# Initialize our linear regression model\n",
    "first_model = LinearRegression()\n",
    "\n",
    "# X is a matrix of inputs, Y is the variable we are trying to learn\n",
    "feature_cols = ['LotArea', 'Utilities', 'OverallCond', 'BsmtFinSF1', 'MasVnrArea']\n",
    "X, Y = get_features(clean, feature_cols, 'SalePrice')\n",
    "\n",
    "# Fit the model to the data\n",
    "first_model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='v'></a>\n",
    "# V. Model Evaluation\n",
    "\n",
    "Now it's time to actually see how your model performed!\n",
    "\n",
    "Just mess around with the `.predict` method of your model object. See what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our prediction for a house with 0 LotArea, 0 Utilities, and a rating of 0 OverallCond is:\n",
      "-29399.97\n"
     ]
    }
   ],
   "source": [
    "# for example\n",
    "prediction = first_model.predict(np.zeros((1,len(feature_cols))))\n",
    "print(\"Our prediction for a house with 0 LotArea, 0 Utilities, and a rating of 0 OverallCond is:\\n{:.2f}\".format(prediction[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our bias is pretty damn high. Obviously, we are extrapolating out of our dataset here, but it immediately gives us some intuition as to what the average price looks like. Actually that probably isn't true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r^2 coeff: 0.329\n",
      "bias: -29399.97\n"
     ]
    }
   ],
   "source": [
    "r2_coeff = first_model.score(X, Y)\n",
    "bias = first_model.intercept_\n",
    "print(\"r^2 coeff: {:.3f}\".format(r2_coeff))\n",
    "print(\"bias: {:.2f}\".format(bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the actual loss of the model, we'll compute the mean squared error on the train dataset. **NOTE:** Be wary here of overfitting. We are training and evaluating our data on the same dataset. However, we are using linear models which are too simple to overfit our data, so it makes a decent way to introduce modeling. In the future, ~20% of the dataset should be set aside for evaluating the model. This is to make sure models *generalize* their predictions.\n",
    "\n",
    "So how does your collection of selected features perform on predicting `SalePrice` using a multivariate linear model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error loss of our model: 4215215723.11\n"
     ]
    }
   ],
   "source": [
    "def get_loss(model, data, col_list, true_col_name):\n",
    "    \"\"\"Returns L2 loss between Y_hat and true values\n",
    "    \n",
    "    model(Model object): model we use to predict values\n",
    "    data(DataFrame): where we get our data from\n",
    "    col_list(list): list of column names that our model uses to predict on\n",
    "    true_col_name(String): name of the column in data we wish to predict\n",
    "    \"\"\"\n",
    "    X, Y_true = get_features(data, col_list, true_col_name)\n",
    "    Y_hat = model.predict(X)\n",
    "    return np.mean((Y_true-Y_hat)**2)\n",
    "\n",
    "loss = get_loss(model=first_model, data=clean, col_list=feature_cols, true_col_name='SalePrice')\n",
    "print(\"Mean Squared Error loss of our model: {:.2f}\".format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusion'></a>\n",
    "# Conclusion\n",
    "\n",
    "This ends the first of our four collaborative sessions on the Kaggle Housing Prices competition. This week or the next would be a good time to come to a SUSA Office Hour and finish cleaning your dataset and getting more performant feature selections. This project will take much more work than was covered in this first hour, but as always, please email [`contact@arun.run`](mailto:contact@arun.run), [`prc@berkeley.edu`](mailto:prc@berkeley.edu), or [Noah Gundotra](mailto:noah.gundotra@berkeley.edu) or with any questions or concerns whatsoever. Happy machine learning!\n",
    "\n",
    "## Sneakpeek at SUSA Kaggle Competition II\n",
    "\n",
    "Next week, we're going to continue our initial modeling of the House Prices dataset with more EDA, a new algorithm for guaranteeing lack of colinearity called **Principal Component Analysis**, and cover concepts in **feature engineering**. The models we will be focusing on next week are **Polynomial Regression** and **Regularized Regression**. Stay tuned!\n",
    "\n",
    "<a id='reading'></a>\n",
    "# Additional Reading\n",
    "* For more information on the Kaggle API, a command-line program used to download and manage Kaggle datasets, visit the [Kaggle API Github page](https://github.com/Kaggle/kaggle-api)  \n",
    "* For an interactive guide to learning R and Python, visit [DataCamp](https://www.datacamp.com/) a paid tutorial website for learning data computing.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
