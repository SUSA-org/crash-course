---
title: SUSA Kaggle Capstone
subtitle: "Part 1: Exploratory Data Analysis and Regression" 
author: "Hosted by and maintained by the [Statistics Undergraduate Students Association (SUSA)](https://susa.berkeley.edu). Originally authored by [Arun Ramamurthy](mailto:contact@arun.run) & [Patrick Chao](mailto:prc@berkeley.edu)."
output: 
  html_document:
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: true
---


```{r, echo = F, message = F, warning = F}
library(tidyverse);library(magrittr);library(ggplot2);library(anytime);library(broom);library(stringr);library(ggfortify);library(GGally);library(gridExtra)

outdir = "~/Desktop/WORKSHOP/C/SUSA/susa-repos/crash-course/Kaggle/GRAPHICS/"
printer = function(filename, w = 16, h = 9) {ggsave(paste0(filename, ".png"), path = outdir, units='in', width = w, height = h, dpi=600, limitsize = F)}
```


# Introduction

Welcome to the first of SUSA's crash courses on advanced data science in Python and R! In this tutorial, you will be learning about one of the most popular and versatile machine learning algorithms, **neural nets**, by using a package called **`Keras`** to implement a neural net model to recognize handwritten digits. We will be guiding you through an entire machine learning workflow, including exploratory data analysis (*EDA*), data cleaning, and the three components of modeling: **model training**, **model selection**, and **model evaluation**. We will even teach you a couple of neat tricks on how to visualize your models to understand their behavior and performance.

## About this Document
### Prerequisites
This workshop prerequires either experience in Python or R, to the degree covered by the previous [SUSA crash courses](http://github.com/SUSA-org/crash-course) - we recommend the completion of the introductory workbook series in both. This is partly to ensure you have the prior data wrangling and programming experience to understand the `Keras` code chunks at the end of this tutorial, but also to ensure you are caught up on the basics of machine learning practices covered incrementally in each workbook. At a minimum, you should understand the purposes of **training** and **validation**, the difference between **classification** and **regression**, and the **bias-variance tradeoff**. You must have RStudio, Python, and R installed to run this notebook. Visit [py0](https://raw.githubusercontent.com/SUSA-org/crash-course/master/Python/py0.html) for our crash course installation guide for Python. Visit [r0](r0.html) for general information on the philosophy and functionality of R and RStudio, as well as installation guides for both.

### adv-pyr1

This document contains textbook-style information on deep learning, as well as R programming as applied to deep learning models. It will cover the entirety of a `Keras` implementation of a neural net to classify handwritten digits. The Python implementation can be viewed in the [adv-pyr1.ipynb](adv-pyr1.ipynb) notebook. The non-programmatic portions of these two documents are identical.

Throughout this tutorial, you will be working with the [**MNIST dataset**](https://en.wikipedia.org/wiki/MNIST_database), a dataset of seventy-thousand images of handwritten digits. You will be using this dataset to train a model to recognize the written digit given by a vector representing the image. More information on this dataset and its structure will be provided early in this tutorial.

Unlike some of the previous crash courses, this workbook will contain discussion and programming exercises throughout each section. These exercises are aimed at facilitating collaborative learning with your committee and target the core concepts and extensions of the techniques covered in this tutorial.

# Kaggle

## What is Kaggle?

## Machine Learning

## Accessing the `House Prices` Dataset

`crash-course/Kaggle/DATA/house-prices/train.csv`

# Exploratory Data Analysis

## The Data Science Workflow

## EDA

```{r}
training_set <- read_csv("Kaggle/DATA/house-prices/train.csv") 
```


```{r}
training_set %>% summarise_all(funs(sum(is.na(.))/nrow(training_set))) %>% gather(Column, Count) %>% arrange(desc(Count)) %>% head(10)
eda_set <- training_set %>% drop_na(-c(PoolQC, MiscFeature, Alley, Fence, FireplaceQu, LotFrontage)) 
nrow(eda_set)/nrow(training_set)
```

```{r}
pairs_plot <- function(df){
  df_x <- rename_all(df, function(str){paste(str, "x", sep = ".")})
  df_y <- rename_all(df, function(str){paste(str, "y", sep = ".")})
  cbind(df_x,df_y) %>%
    gather(xVar, xVal, ends_with("x")) %>% gather(yVar, yVal, ends_with("y")) %>% 
    mutate_at(vars(xVar, yVar), function(str){str_sub(str, 1, nchar(str) - 2)}) %>%
    ggplot(aes(xVal, yVal, col = xVar)) + geom_point() + facet_grid(yVar ~ xVar, scales = "free") + 
    labs(x = "X Variable", y = "Y Variable")
}
pairs_plot(eda_set %>% select_if(is.numeric))
eda_set %>% `names<-`(make.names(names(.))) %>% 
  select_if(is.numeric) %>% mutate(BinaryQuality = if_else(OverallQual >= 7, 1, -1) %>% as.factor) %>% 
  select(find_high_corr(select(., - BinaryQuality)), BinaryQuality) %>% `[`(1:3) %>%
  ggpairs(lower = list(continuous = "smooth_loess"), 
          axisLabels = "none", switch = "both", labeller = label_wrap_gen(), 
          aes(col = BinaryQuality, alpha = .2))
printer("pairs")
```

```{r}
corr_plot <- function(df) {
  df %>% cor %>% as.data.frame %>% gather(xVar, Corr) %>% mutate(yVar = rep(names(df), times = length(df))) %>%
    ggplot(aes(xVar, yVar, fill = Corr)) + geom_tile() + scale_fill_distiller(type = "div", palette = 2, limits = c(-1, 1)) +
    labs(x = "X Variable", y = "Y Variable") + theme(axis.text.x = element_text(angle = 90))
}
find_high_corr <- function(df, corr = .6) {
  cor_df <- df %>% cor %>% as.data.frame %>% gather(xVar, Corr) %>% mutate(yVar = rep(names(df), times = length(df))) %>%
    filter(! Corr %in% c(1, -1), Corr >= corr) %>% arrange(desc(Corr)) 
  return(c(cor_df$xVar, cor_df$yVar) %>% unique)
}
find_high_corr(eda_set %>% select_if(is.numeric)) 
corr_plot(eda_set %>% select_if(is.numeric))
pairs_plot(eda_set %>% select(TotalBsmtSF, `1stFlrSF`))
```

```{r}
density_plots <- function(df) {
  df %>% gather(Variable, Value) %>% 
    ggplot(aes(Value, fill = Variable, col = Variable)) + geom_density() + facet_wrap(~ Variable, scales = "free") + 
    theme(legend.position = "none") + labs(x = "", y = "")
}
density_plots(eda_set %>% select_if(is.numeric))
```


```{r}
pca_plot <- function(df, scale = F) {
  autoplot(prcomp(df, scale = scale), data = df, 
         loadings = TRUE, loadings.colour = 'blue',
         loadings.label = TRUE, loadings.label.size = 3)
}
pca_set <- eda_set %>% select(- SalePrice) %>% select_if(is.numeric) %>% drop_na
pca_plot(pca_set)
eda_pca <- prcomp(pca_set)
summary(eda_pca)
eda_pca %>% extract(c("sdev", "center")) %>% as.data.frame %>% mutate(Feature = rownames(.)) %>% arrange(desc(center)) %>% 
  slice(1:20) %>% 
  pull(Feature) -> important_features
print(important_features)
  
```

# Feature Selection

# A First Approach to Machine Learning: Linear Regression

```{r}
simple_model <- lm(data = training_set, formula = SalePrice ~ LotArea)
simple_model %>% augment %>% select(`SalePrice (Actual)` = SalePrice, `SalePrice (Predicted)` = .fitted, LotArea) %>%
  gather(Value, SalePrice, - LotArea) %>%
  ggplot(aes(LotArea, SalePrice, col = Value, linetype = Value)) + geom_point()

initial_model <- lm(data = training_set, formula = SalePrice ~ LotArea + GrLivArea)
initial_model %>% augment %>% select(`SalePrice (Actual)` = SalePrice, `SalePrice (Predicted)` = .fitted, LotArea, GrLivArea) %>%
  gather(Value, SalePrice, - c(LotArea, GrLivArea)) %>%
  ggplot(aes(LotArea, SalePrice, col = Value, linetype = Value)) + geom_point()
initial_model %>% augment %>% 
  ggplot(aes(LotArea, .resid)) + geom_point() + 
  geom_hline(yintercept = 0, col = "black", linetype = "dashed") +
  labs(x = "Lot Area (Square Ft.)", y = "Residual", title = "Residual Plot of Iniital Linear Model") 

model <- lm(data = training_set, 
            formula = as.formula(paste("SalePrice ~ ", paste(important_features %>% paste0("`", ., "`"), collapse= " + "))))
model %>% augment %>% 
  ggplot(aes(SalePrice, .resid)) + geom_point() + 
  geom_hline(yintercept = 0, col = "black", linetype = "dashed") +
  labs(x = "Sales Price", y = "Residual", title = "Residual Plot of 'Important Features' Linear Model") 
```


# Extensions to Linear Regression

# Conclusion

This ends our textbook-style primer into deep learning with Keras. While this was just an introduction to neural nets, we hope that you can now see some of the workflow patterns associated with machine learning. Feel free to play around with the code above to get a better feel for the hyperparameters of the neural net model. As always, please email [`contact@arun.run`](mailto:contact@arun.run) or [`prc@berkeley.edu`](mailto:prc@berkeley.edu) with any questions or concerns whatsoever. Happy machine learning!

## Sneakpeek at SUSA Kaggle Competition II

After Spring Break, we will be guiding you through a four-week collaborative Kaggle competition with your peers in Career Exploration! We want to give you the experience of working with real data, using real machine learning algorithms, in an educational setting. You will have to choose either Python or R, and dive into reading kernels on the Kaggle website, use visualization and feature engineering to improve your score, and maybe even pick up a few advanced deep learning models along the way. If this sounds a bit intimidating right now, do not fret! Your SUSA Mentors will be there to mentor you through the whole thing. So rest up during Spring Break, and come back ready to tackle your biggest data challenge yet!

# Additional Reading
* For more information on the Kaggle API, a command-line program used to download and manage Kaggle datasets, visit the [Kaggle API Github page](https://github.com/Kaggle/kaggle-api)  
* For an interactive guide to learning R and Python, visit [DataCamp](https://www.datacamp.com/) a paid tutorial website for learning data computing.
