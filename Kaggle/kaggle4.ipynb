{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUSA CX Kaggle Capstone Project\n",
    "## Part 4: Deep Learning in Keras and Submitting to Kaggle\n",
    "\n",
    "### Table Of Contents\n",
    "* [Introduction](#section1)\n",
    "* [Initial Setup](#section2)\n",
    "* [Deep Learning](#section3)\n",
    "* [Final Kaggle Evaluation](#section4)   \n",
    "* [Conclusion](#conclusion)\n",
    "* [Additional Reading](#reading)\n",
    "\n",
    "\n",
    "### Hosted by and maintained by the [Statistics Undergraduate Students Association (SUSA)](https://susa.berkeley.edu). Originally authored by [Patrick Chao](mailto:prc@berkeley.edu) & [Arun Ramamurthy](mailto:contact@arun.run)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "# SUSA CX Kaggle Capstone Project\n",
    "\n",
    "Woohoo! You've made it to the end of the CX Kaggle Capstone Project! Congratulations on all of your hard work so far. We hope you've enjoyed this opportunity to learn new modeling techniques, some underlying mathematics, and even make new friends within CX. At this point, we've covered the entirety of the Data Science Workflow, linear regression, feature engineering, PCA, shrinkage, hyperparameter tuning, decision trees and even ensemble models. This week, we're going to finish off this whirlwind tour with a revisit to our old friend, Deep Learning. While the MNIST digit dataset was really interesting to look at as a cool toy example of the powers of DL, this time you're going to apply neural networks to your housing dataset for some hands-on practice using Keras. \n",
    "\n",
    "> ### CX Kaggle Competition & Final Kaggle Evaluation\n",
    "After you get some practice with deep learning, this week we will be asking you and your team to select and finalize your best model, giving you the codespace to write up your finalized model and evaluate it by officially submitting your results to Kaggle. The winners of this friendly collab-etition will be honored at the SUSA Banquet next Friday, including prizes for the winning team! We also want to encourage and facilitate discussion between teams on why different models performed differently, and give you a chance to chat with other teams about their own experiences with the CX Kaggle Capstone. \n",
    "\n",
    "## Logistics\n",
    "\n",
    "Most of the logistics are the same as last week, but we are repeating them here for your convenience. Please let us know if you or your teammates are feeling nervous about the pace of this project - remember that we are not grading you on your project, and we really try to make the notebooks relatively easy and fast to code through. If for any reason you are feeling overwhelmed or frustrated, please DM us or talk to us in person. We want all of you to have a productive, healthy, and fun time learning data science! If you have any suggestions or recommendations on how to improve, please do not hesitate to reach out!\n",
    "\n",
    "\n",
    "### Mandatory Office Hours\n",
    "\n",
    "Because this is such a large project, you and your team will surely have to work on it outside of meetings. In order to get you guys to seek help from this project, we are making it **mandatory** for you and your group to attend **two (2)** SUSA Office Hours over the next 4 weeks. This will allow questions to be answered outside of the regular meetings and will help promote collaboration with more experienced SUSA members.\n",
    "\n",
    "The schedule of SUSA office hours are below:\n",
    "https://susa.berkeley.edu/calendar#officehours-table\n",
    "\n",
    "We understand that most of you will end up going to Arun or Patrick's office hours, but we highly encourage you to go to other people's office hours as well. There are many qualified SUSA mentors who can help and this could be an opportunity for you to meet them.\n",
    "\n",
    "<a id='section2'></a>\n",
    "# Initial Setup\n",
    "\n",
    "To begin we will import all the necessary libraries and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "from sklearn import tree # There are lots of other models from this module you can try!\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression, Ridge\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Activation\n",
    "from keras import backend as K\n",
    "sqrt=np.sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a few familiar functions that should be helpful to you later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(data, col_list, y_name):\n",
    "    \"\"\"\n",
    "    Function to return a numpy matrix of pandas dataframe features, given k column names and a single y column\n",
    "    Outputs X, a n X k dimensional numpy matrix, and Y, an n X 1 dimensional numpy matrix.\n",
    "    This is not a smart function - although it does drop rows with NA values. It might break. \n",
    "    \n",
    "    data(DataFrame): e.g. train, clean\n",
    "    col_list(list): list of columns to extract data from\n",
    "    y_name(string): name of the column you to treat as the y column\n",
    "    \n",
    "    Ideally returns one np.array of shape (len(data), len(col_list)), and one of shape (len(data), len(col_list))\n",
    "    \"\"\"\n",
    "    \n",
    "    # keep track of numpy values\n",
    "    feature_matrix = data[col_list + [y_name]].dropna().values\n",
    "    np.random.shuffle(feature_matrix)\n",
    "    return feature_matrix[:, :-1], feature_matrix[:, -1]\n",
    "\n",
    "def get_loss(model, X,Y_true):\n",
    "    \"\"\"Returns square root of L2 loss (RMSE) from a model, X value input, and true y values\n",
    "    \n",
    "    model(Model object): model we use to predict values\n",
    "    X: numpy matrix of x values\n",
    "    Y_true: numpy matrix of true y values\n",
    "    \"\"\"\n",
    "    Y_hat = model.predict(X)\n",
    "    return get_RMSE(Y_hat,Y_true)\n",
    "\n",
    "def get_RMSE(Y_hat,Y_true):\n",
    "    \"\"\"Returns square root of L2 loss (RMSE) between Y_hat and true values\n",
    "    \n",
    "    Y_true: numpy matrix of predicted y values\n",
    "    Y_true: numpy matrix of true y values\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.mean((Y_true-Y_hat)**2))\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "def get_train_and_val(X,Y):\n",
    "    \"\"\"Given the X and Y data, return the training and validation based on the split variable\n",
    "    \n",
    "    X: numpy matrix of x values\n",
    "    Y: numpy matrix of y values\n",
    "    split: value between 0 and 1 for the training split\n",
    "    \"\"\"\n",
    "    \n",
    "    Y = Y.reshape(Y.shape[0],)\n",
    "\n",
    "    train_index,_ = get_train_val_indices(X,Y)\n",
    "\n",
    "    y_train = Y.reshape(Y.shape[0],)\n",
    "    y_train = Y[:train_index]\n",
    "    x_train = X[:train_index,:]\n",
    "\n",
    "    x_val = X[train_index:,:]\n",
    "    y_val = Y[train_index:]\n",
    "    return (x_train,y_train),(x_val,y_val)\n",
    "\n",
    "def get_train_val_indices(X,Y=None,split=0.7):\n",
    "    train_index = (int)(X.shape[0]*split)\n",
    "    test_index =X.shape[0]-1\n",
    "    return train_index,test_index\n",
    "\n",
    "def select_columns_except(dframe, non_examples):\n",
    "    \"\"\"Returns all comlumns in dframe except those in non_examples.\"\"\"\n",
    "    all_cols = dframe.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cond = lambda x: sum([x == col for col in non_examples]) >= 1\n",
    "    return [x for x in all_cols if not cond(x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "First, we need to load and clean the data. Although you may have cleaned data from `kaggle1`, we provide our solution for the cleaned housing data for your convenience. If you would like to view the completed data cleaning procedure, it has been updated in [kaggle1.ipynb](kaggle1.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>BsmtCond</th>\n",
       "      <th>BsmtExposure</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>BsmtFinType1</th>\n",
       "      <th>BsmtFinType2</th>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <th>...</th>\n",
       "      <th>ExteriorWd Shng</th>\n",
       "      <th>ExteriorImStucc</th>\n",
       "      <th>ExteriorWdShing</th>\n",
       "      <th>ExteriorMetalSd</th>\n",
       "      <th>ExteriorCmentBd</th>\n",
       "      <th>ExteriorCemntBd</th>\n",
       "      <th>ExteriorPlywood</th>\n",
       "      <th>ExteriorBrkFace</th>\n",
       "      <th>ExteriorWd Sdng</th>\n",
       "      <th>ExteriorBrk Cmn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>856</td>\n",
       "      <td>854</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>706.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>920</td>\n",
       "      <td>866</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>486.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>961</td>\n",
       "      <td>756</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>216.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1145</td>\n",
       "      <td>1053</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>655.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 168 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1stFlrSF  2ndFlrSF  BedroomAbvGr  BsmtCond  BsmtExposure  BsmtFinSF1  \\\n",
       "0       856       854             3         3             1       706.0   \n",
       "1      1262         0             3         3             4       978.0   \n",
       "2       920       866             3         3             2       486.0   \n",
       "3       961       756             3         4             1       216.0   \n",
       "4      1145      1053             4         3             3       655.0   \n",
       "\n",
       "   BsmtFinSF2  BsmtFinType1  BsmtFinType2  BsmtFullBath       ...         \\\n",
       "0         0.0             6             1           1.0       ...          \n",
       "1         0.0             5             1           0.0       ...          \n",
       "2         0.0             6             1           1.0       ...          \n",
       "3         0.0             5             1           1.0       ...          \n",
       "4         0.0             6             1           1.0       ...          \n",
       "\n",
       "   ExteriorWd Shng  ExteriorImStucc  ExteriorWdShing  ExteriorMetalSd  \\\n",
       "0                0                0                0                0   \n",
       "1                0                0                0                1   \n",
       "2                0                0                0                0   \n",
       "3                1                0                0                0   \n",
       "4                0                0                0                0   \n",
       "\n",
       "   ExteriorCmentBd  ExteriorCemntBd  ExteriorPlywood  ExteriorBrkFace  \\\n",
       "0                0                0                0                0   \n",
       "1                0                0                0                0   \n",
       "2                0                0                0                0   \n",
       "3                0                0                0                0   \n",
       "4                0                0                0                0   \n",
       "\n",
       "   ExteriorWd Sdng  ExteriorBrk Cmn  \n",
       "0                0                0  \n",
       "1                0                0  \n",
       "2                0                0  \n",
       "3                1                0  \n",
       "4                0                0  \n",
       "\n",
       "[5 rows x 168 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('DATA/house-prices/train_cleaned.csv')\n",
    "test = pd.read_csv('DATA/house-prices/test_cleaned.csv')\n",
    "train = train.drop('Unnamed: 0',axis=1)\n",
    "test = test.drop('Unnamed: 0',axis=1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as before, we need to preprocess the data into `numpy` matrices and separate the `SalePrice` as the response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = select_columns_except(train, ['Id','SalePrice'])\n",
    "\n",
    "X, Y = get_features(train, feature_cols, 'SalePrice')\n",
    "(x_train,y_train),(x_val,y_val) = get_train_and_val(X,Y)\n",
    "\n",
    "x_test = test.loc[:, test.columns != 'Id'].values\n",
    "test_ids = test['Id'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a function `model_prediction` that takes in a model and a set of features from the test set, and outputs the predictions into a vector. This should work with the `keras` neural networks, `sklearn` decision trees and random forests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_prediction(model,test=x_test):\n",
    "    prediction = model.predict(test)\n",
    "    return prediction.reshape(prediction.shape[0],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Example 1 : Random Forest\n",
    "To help you get started, we supply a couple of naive models. Recall the three steps to modeling: model selection, training, and evaluation (validation or testing). Use the optimal parameters you found from grid search last week to tune the following random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "### MODEL DESIGN ###\n",
    "####################\n",
    "max_depth = 10 \n",
    "min_samples_leaf = 1 \n",
    "min_samples_split = 4\n",
    "n_estimators = 40\n",
    "model_rf = RandomForestRegressor(max_depth = max_depth,\n",
    "                              min_samples_leaf = min_samples_leaf, min_samples_split = min_samples_split,\n",
    "                              n_estimators = n_estimators, random_state = 0, bootstrap = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "### TRAINING ###\n",
    "################\n",
    "model_rf = model_rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error loss on the Validation Set for our RF model: 26191.52\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "### EVALUATION ###\n",
    "##################\n",
    "loss = get_loss(model_rf, x_val,y_val)\n",
    "print(\"Root Mean Squared Error loss on the Validation Set for our RF model: {:.2f}\".format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "# Deep Learning\n",
    "\n",
    "From `kaggle3`:  \n",
    ">We may imagine hyperparameters as a bunch of individual knobs we can turn to change our model. Consider that you are visiting your friend and staying at her place. However, you did not realize that she is actually an alien and her house is filled with very strange objects. When you head to bed, you attempt to use her shower, but see that her shower is has a dozen of knobs that control the temperature of the water coming out! We only have a single output to work off of, but many different knobs or *parameters* to adjust. If the water is too hot, we can turn random knobs until it becomes cold, and learn a bit about our environment. We may determine that some knobs are more or less sensitive, just like hyperparameters. Each knob in the shower is equivalent to a hyperparameter we can tune in a model.\n",
    "\n",
    "## Model Example 2 : Neural Networks\n",
    "Here is a very simple example of a neural network in Keras. It's performance is very poor to start, but mess around with tuning parameters and adding or subtracting layers and see what you can come up with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "### MODEL DESIGN ###\n",
    "####################\n",
    "model_nn = Sequential()\n",
    "model_nn.add(Dense(30, activation='relu', input_shape=(x_train.shape[1],)))\n",
    "model_nn.add(Dense(1, activation='relu'))\n",
    "\n",
    "model_nn.compile(optimizer=Adam(), loss = root_mean_squared_error, \n",
    "              metrics =[root_mean_squared_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "################\n",
    "### TRAINING ###\n",
    "################\n",
    "batch_size = 20\n",
    "epochs = 50\n",
    "learning_rate = 0.01\n",
    "history = model_nn.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error loss on the Validation Set for our neural net model: 179413.74\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "### EVALUATION ###\n",
    "##################\n",
    "score = model_nn.evaluate(x_val, y_val, verbose=0)\n",
    "print(\"Root Mean Squared Error loss on the Validation Set for our neural net model: {:.2f}\".format(score[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "# Final Kaggle Evaluation\n",
    "\n",
    "Congrats on finishing the last of the models we planned on teaching you about during the CX Kaggle Capstone! \n",
    "\n",
    "You have now covered five distinct models and a several related techniques to add to your data science bag-of-tricks: \n",
    "- Linear Models\n",
    "    - Multivariate Linear Regression\n",
    "    - Polynomial Regression\n",
    "    - Shrinkage / Biased Regression / Regularization (i.e. Ridge, LASSO)\n",
    "- Decision Trees\n",
    "    - Random Forests\n",
    "- Deep Learning\n",
    "    - Sequential Neural Networks\n",
    "- Auxiliary Techniques \n",
    "    - The Data Science Workflow\n",
    "    - Data Cleaning\n",
    "    - Interpreting EDA Graphs\n",
    "    - Feature Engineering\n",
    "    - Principal Component Analysis\n",
    "    - Hyperparameter Tuning (i.e. grid search)\n",
    "    - Ensemble Learning (i.e. bagging, boosting) \n",
    "    \n",
    "Wow, that's a lot! We are really proud of you all for exploring these techniques, which constitute some of Berkeley's toughest machine learning and statistics classes. As always, if you want to learn more about any of these topics, or are hungry to learn about even more techniques, feel free to reach out to any one of the SUSA Mentors.\n",
    "\n",
    "With the help of the above listing and your own team's preferences, choose a model and a couple of techniques to implement for your final model. We will provide you with a preamble and some space to construct and train your model, as well as a helper function to turn your output into an official Kaggle submission file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "### PREAMBLE ###\n",
    "################\n",
    "train = x_train\n",
    "labels = y_train\n",
    "test = x_test \n",
    "\n",
    "####################\n",
    "### MODEL DESIGN ###\n",
    "####################\n",
    "model = model_rf\n",
    "# ^^ REPLACE THIS LINE ^^\n",
    "\n",
    "################\n",
    "### TRAINING ###\n",
    "################\n",
    "model = model.fit(train, labels)\n",
    "# ^^ REPLACE THIS LINE ^^\n",
    "\n",
    "##################\n",
    "### EVALUATION ###\n",
    "##################\n",
    "test_predictions = model_prediction(model)\n",
    "\n",
    "##################\n",
    "### SUBMISSION ###\n",
    "##################\n",
    "def generate_kaggle_submission(predictions, test = test):\n",
    "    '''\n",
    "    This function accepts your 1459-dimensional vector of predicted SalesPrices for the test dataset, \n",
    "    and writes a CSV named kaggle_submission.csv containing your vector in a form suitable for \n",
    "    submission onto the Kaggle leaderboard.\n",
    "    '''\n",
    "    pd.DataFrame({'Id': test_ids, 'SalePrice': predictions}) \\\n",
    "      .to_csv('kaggle_submission.csv', index=False)\n",
    "    \n",
    "generate_kaggle_submission(test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might have noticed in the code block above, we had to write a simple CSV file containing row IDs and predicted values for the 1459 houses in the test dataset. This submission file is your ticket to getting onto the official Kaggle leaderboard and seeing how you did as compared to the rest of the world! \n",
    "\n",
    "Take a look at your `kaggle_submission.csv` file to ensure its content matches your expectations. When you and your team are ready, follow these instructions to upload your predictions to Kaggle and receive an official Kaggle score:\n",
    "\n",
    "> 1. First, choose one person on your team (perhaps the rprincess) to submit your team's predictions under their name. This person will need to visit the [Kaggle website](https://www.kaggle.com) to create an account.\n",
    "> 2. Go to the [Competition Submission Page](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/submit) for the House Prices competition. \n",
    "> 3. Upload and submit `kaggle_submission.csv`\n",
    "> 4. Wait for your submission to be scored! Have some snacks, talk to some of your friends in CX, and refresh the page after a few minutes to see your score.\n",
    "> 5. To each member: record your team's score in [this Google form](TODO), which also contains a section for feedback on your Kaggle experience this semester. Congratulations on your first Kaggle submission!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For initial comparison, the naive random forest in this notebook acheives a score of 0.155 - you can think of this as a baseline score to beat.   \n",
    "\n",
    "This is Patrick's score after some minor edits to the models above:\n",
    "\n",
    "<img src=\"GRAPHICS/kaggle.png\" width=\"80%\">\n",
    "\n",
    "Can you beat Patrick's Kaggle score?\n",
    "> \"As an incentive, if you beat the score of 0.13, I will personally take your team out to lunch!\" - Patrick Chao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will post a leaderboard of all of the CX Kaggle teams, and the winning team will be honored at banquet, so stay tuned for that! \n",
    "\n",
    "In the meantime, take some time to talk to other finished teams and explore the differences in your modeling approach, design, and insights! Happy Kaggling :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This brings us to an end to the CX Kaggle Capstone Project, as well as the Spring 2018 semester of SUSA Career Exploration. Congratulations on graduating from the SUSA Career Exploration committee! It's been a wonderful experience teaching you all, and we hope you got as much out of CX as we did this semester. This semester brought several new pilot programs to CX, such as the crash courses, workbooks, a revamped curriculum, and the CX Kaggle Capstone Project. You all have been great sources of feedback, and we want to make next semester's CX curriculum even better for the new generation of CX! \n",
    "\n",
    "We're going to ask you for feedback one last time, to give us insight into how we can improve the CX Kaggle Capstone experience for future CX members. Please fill out [this feedback form](TODO) and let us know how we could have done better. Thank you again for a wonderful semester, and we will see you again in the Fall!\n",
    "\n",
    "\n",
    "\n",
    "As always, please email [Arun Ramamurthy](mailto:contact@arun.run), [Patrick Chao](mailto:prc@berkeley.edu), or [Noah Gundotra](mailto:noah.gundotra@berkeley.edu) with any questions or concerns whatsoever. Have a great summer, and we hope to see you as a returning member in the Fall! Go SUSA!!!\n",
    "\n",
    "With geom_love,  \n",
    "Lucas, Arun, Patrick, Noah, and the rest of the SUSA Board"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
